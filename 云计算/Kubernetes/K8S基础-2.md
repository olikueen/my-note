[toc]

# 5. 资源清单

 ## 5.1 K8S中的资源

K8s 中所有的内容都抽象为资源，资源实例化后，叫做对象；

### 5.1.1 集群资源分类

> 名称空间级别：仅在此名称空间下生效

- 工作负载型资源（workload）：Pod, Deployment, StatefulSet, DaemonSet, Job, CronJob（ReplicationController在 v1.11 版本中被废弃）
- 服务发现及负载均衡型资源（ServiceDiscovery LoadBlance）：Service, Ingress, ...
- 配置与存储型资源：Volume（存储卷）, CSI（容器存储接口，可以扩展各种各样格式的第三方存储卷）
- 特殊类型的存储卷：ConfigMap（当配置中心来使用的资源类型）, Secret（保存敏感数据）, DownwardAPI（把外部环境中的信息输出给容器）

> 集群级资源

Namespace、Node、Role、ClusterRole、RoleBinding、ClusterRoleBinding

> 元数据型资源

HPA、PodTemplate、LimitRange



## 5.2 资源清单

> 资源清单含义

在K8s中，一般使用 yaml 格式的文件来创建符合预期期望的 Pod，这样的 yaml 文件一般称为资源清单；



> yaml 语法

```yaml
基本语法：
	缩进时不允许使用Tab键，只运行使用空格；
	缩进的空格数目不重要，只要相同层级的元素左侧对齐即可；
	# 标识注释，从这个字符一直到行位，都会被解释器忽略；

=============================================================================
数据结构：
	对象：键值对的集合，又称为映射（mapping）/哈希（hashes）/字典（dictionary）
	数组：一组按次序排列的值，又称为序列（sequence）/列表（list）
	纯量（scalars）：单个的、不可再分的值

=============================================================================
对象类型：
	对象的一组键值对，使用冒号结构表示
        name: Bob
        age: 18
    yaml也允许另一种写法，将所有键值对写成一个行内对象
    	hash: {name: Bob, age: 18}
数组类型：
	一组 - 开头的行，构成一个数组
		animal
		- cat
		- dog
	数组也可以采用行内表示法
		animal: [cat, dog]
复合结构：
	对象和数组可以结合使用，形成复合结构
		language：
		- java
		- python
		- golang
		websites:
		baidu: www.baidu.com
		google: www.google.com
		jd: www.jd.com
纯量：
	纯量是最基本的、不可再分的值；以下数据类型都属于纯量
		字符串、布尔值、整数、浮点数、null
		时间、日期
	数值直接以字面量的形式表示
		age: 18
	布尔值用true和false表示
		isSet: true
	null用 ~ 表示
		price: ~
	时间采用iso8601格式
		now: 2020-05-04t15:20:43.10-05:00
	日期采用复合iso8601格式的年月日表示
		date: 2020-05-04
	yaml 允许使用两个感叹号，强制转换数据类型
		e: !!str 123
		f: !!str true
```





## 5.3 常用字段解释说明

查看资源文档`kubectl explain <resource>`

#### 常用字段

**\* 必须存在的属性**

| 参数名                 | 字段类型 | 说明                                                         |
| ---------------------- | -------- | ------------------------------------------------------------ |
| version                | String   | K8S API 的版本, 目前是基于v1, 可以用 kubectl api -version 命令查询 |
| kind                   | String   | 指 yaml 文件定义的资源类型和角色, 如: Pod                    |
| metadata               | Object   | 元数据对象, 固定值写 metadata                                |
| metadata.name          | String   | 元数据对象的名字, 自定义, 比如命名Pod的名字                  |
| metadata.namespace     | String   | 元数据对象的命名空间, 自定义                                 |
| spce                   | Object   | 详细定义对象, 固定值写Spec                                   |
| spce.container[]       | list     | Spec对象的容器列表定义, 是个列表                             |
| spce.container[].name  | String   | 定义容器的名字                                               |
| spce.container[].image | String   | 定义要用到的镜像名称                                         |

**\* spec 主要对象**

| 参数名                                     | 字段类型 | 说明                                                         |
| ------------------------------------------ | -------- | ------------------------------------------------------------ |
| spec.containers[].name                     | String   | 定义容器的名字                                               |
| spec.containers[].image                    | String   | 定义要用到的镜像的名称                                       |
| spec.containers[].imagePullPolicy          | String   | 定义镜像拉取策略, 有Always, Never, IfNotPresent三个值可选<br>(1)Always: 每次构建都尝试重新拉取镜像<br>(2)Never: 表示仅使用本地镜像<br>(3)IfNotPresent: 如果本地有镜像就用本地镜像, 没有就拉取在线镜像<br>上面三个值都没设置的话, 默认是Always |
| spec.containers[].command[]                | List     | 指定容器启动命令, 因为是数组可以指定多个, 不指定则适用镜像打包时使用的启动命令 |
| spec.containers[].args[]                   | List     | 指定容器启动命令的参数, 因为是数组可以指定多个               |
| spec.containers[].workingDir               | String   | 指定容器的工作目录                                           |
| spec.containers[].volumeMounts[]           | List     | 指定容器内部的存储卷配置                                     |
| spec.containers[].volumeMounts[].name      | String   | 指定可以被容器挂载的存储卷的名称                             |
| spec.containers[].volumeMounts[].mountPath | String   | 指定可以被容器挂载的容器卷的路径                             |
| spec.containers[].volumeMounts[].readOnly  | String   | 设置存储卷路径的读写模式, true 或者 false, 默认为读写模式    |
| spec.containers[].ports[]                  | List     | 指定容器需要用到的端口列表                                   |
| spec.containers[].ports[].name             | String   | 指定端口名称                                                 |
| spec.containers[].ports[].containerPort    | String   | 指定容器需要监听的端口号                                     |
| spec.containers[].ports.hostPort           | String   | 指定容器所在主机需要监听的端口号, 默认跟 containerPort 相同, 注意设置了 hostPort 同一台主机无法启动该容器的相同副本 (因为主机的端口号不能相同, 这样会冲突) |
| spec.containers[].ports[].portocol         | String   | 指定端口协议, 支持TCP和UDP, 默认值为TCP                      |
| spec.containers[].env[]                    | List     | 指定容器运行需设置的环境变量列表                             |
| spec.containers[].env[].name               | String   | 指定环境变量名称                                             |
| spec.containers[].env[].value              | String   | 指定环境变量值                                               |
| spec.containers[].resource                 | Object   | 指定资源限制和资源请求的值                                   |
| spec.containers[].resource.limits          | Object   | 指定容器运行时资源的运行上限                                 |
| spec.containers[].resource.limits.cpu      | String   | 指定CPU的限制, 单位为 core 数, 将用于 docker run --cpu-share 参数 |
| spec.containers[].resource.memory          | String   | 指定MEM内存的限制, 单位为MIB, GIB                            |
| spec.containers[].resource.requests        | Object   | 指定容器启动和调度室的限制设置                               |
| spec.containers[].resource.requests.cpu    | String   | CPU请求, 单位为 core 数, 容器启动时初始化可用数量            |
| spec.containers[].resource.requests.memory | String   | 内存请求, 单位为 MIB, GIB 容器启动的初始化可用数量           |

**\* 额外的参数**

| 参数名                | 字段类型 | 说明                                                         |
| --------------------- | -------- | ------------------------------------------------------------ |
| spec.restartPolicy    | String   | 定义Pod重启策略, 可选值为 Always、OnFailure、Never, 默认值为Always<br>(1) Always: Pod一旦终止运行, 则无论容器时如何终止的, kubelet服务都将重启它<br>(2) OnFailure: 只有Pod以非零退出码终止时, kubelet才会重启该容器; 如果容器正常退出(退出码为0), 则 kubelet 将不会重启它<br>(3) Never: Pod 终止后, kunelet 将退出码报告给 Master, 不会重启该 Pod |
| spec.nodeSelector     | Object   | 定义 Node 的 Label 过滤标签, 以 key:value 格式指定           |
| spec.imagePullSecrets | Object   | 定义 pull 镜像是使用 secret 名称, 以 name:srcretkey 格式指定 |
| spec.hostNetwork      | Boolean  | 定义是否使用主机网络模式, 默认值为 false; 设置true 表示使用宿主机网络, 不适用 docker网桥, 同时设置了 true 将无法在同一台宿主机启动第二个副本 |

#### 举例说明

>  创建一个namespace

```
apiVersion: v1
kind: Namespace
metadata:
  name: test
```

> demo: myapp.yaml

```
apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
  # namespace: default
  labels:
    app: myapp
    version: v1
spec: # 容器详细信息
  containers: 
  - name: app
    image: nginx:latest
  - name: test
    image: nginx:latest
```

```
# 通过yaml文件创建pod
[root@master 20200528]# kubectl apply -f myapp.yaml

# pod端口冲突无法运行
[root@master 20200528]# kubectl get pod
NAME        READY   STATUS   RESTARTS   AGE
myapp-pod   1/2     Error    1          63s
[root@master 20200528]# kubectl get pod
NAME        READY   STATUS   RESTARTS   AGE
myapp-pod   1/2     Error    1          64s
[root@master 20200528]# kubectl get pod
NAME        READY   STATUS             RESTARTS   AGE
myapp-pod   1/2     CrashLoopBackOff   1          65s

# 查看pod信息
[root@master 20200528]# kubectl describe pod myapp-pod
Name:               myapp-pod
Namespace:          default
Priority:           0
PriorityClassName:  <none>
Node:               node01.alec.com/192.168.30.131
Start Time:         Thu, 28 May 2020 20:53:38 +0800
Labels:             app=myapp
                    version=v1
Annotations:        kubectl.kubernetes.io/last-applied-configuration:
                      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"labels":{"app":"myapp","version":"v1"},"name":"myapp-pod","namespace":"defau...
Status:             Running
IP:                 10.244.1.2
Containers:
  app:
    Container ID:   docker://3a996309d0bd8da0ef46bffe73ab2a3179d31e7615e601d184d076b648849197
    Image:          nginx:latest
    Image ID:       docker-pullable://nginx@sha256:30dfa439718a17baafefadf16c5e7c9d0a1cde97b4fd84f63b69e13513be7097
    Port:           <none>
    Host Port:      <none>
    State:          Running			>>> 运行中
      Started:      Thu, 28 May 2020 20:54:13 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8lcrd (ro)
  test:
    Container ID:   docker://f867f863f5216f0bc0276341db7df5c821a809095688527144e952a04cc13a1c
    Image:          nginx:latest
    Image ID:       docker-pullable://nginx@sha256:30dfa439718a17baafefadf16c5e7c9d0a1cde97b4fd84f63b69e13513be7097
    Port:           <none>
    Host Port:      <none>
    State:          Terminated 		>>> 故障
      Reason:       Error
      Exit Code:    1
      Started:      Thu, 28 May 2020 20:56:30 +0800
      Finished:     Thu, 28 May 2020 20:56:33 +0800
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Thu, 28 May 2020 20:55:34 +0800
      Finished:     Thu, 28 May 2020 20:55:37 +0800
    Ready:          False
    Restart Count:  4
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-8lcrd (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             False 
  ContainersReady   False 
  PodScheduled      True 
Volumes:
  default-token-8lcrd:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-8lcrd
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason     Age                  From                      Message
  ----     ------     ----                 ----                      -------
  Normal   Scheduled  3m5s                 default-scheduler         Successfully assigned default/myapp-pod to node01.alec.com
  Normal   Pulling    2m59s                kubelet, node01.alec.com  pulling image "nginx:latest"
  Normal   Pulled     2m30s                kubelet, node01.alec.com  Successfully pulled image "nginx:latest"
  Normal   Created    2m30s                kubelet, node01.alec.com  Created container
  Normal   Started    2m30s                kubelet, node01.alec.com  Started container
  Normal   Pulling    88s (x4 over 2m30s)  kubelet, node01.alec.com  pulling image "nginx:latest"
  Normal   Pulled     69s (x4 over 2m25s)  kubelet, node01.alec.com  Successfully pulled image "nginx:latest"
  Normal   Created    69s (x4 over 2m24s)  kubelet, node01.alec.com  Created container
  Normal   Started    69s (x4 over 2m24s)  kubelet, node01.alec.com  Started container
  Warning  BackOff    53s (x5 over 2m12s)  kubelet, node01.alec.com  Back-off restarting failed container

# 查看容器日志
[root@master 20200528]# kubectl log myapp-pod
log is DEPRECATED and will be removed in a future version. Use logs instead.
Error from server (BadRequest): a container name must be specified for pod myapp-pod, choose one of: [app test]
[root@master 20200528]# kubectl log myapp-pod -c app
log is DEPRECATED and will be removed in a future version. Use logs instead.
[root@master 20200528]# kubectl log myapp-pod -c test
log is DEPRECATED and will be removed in a future version. Use logs instead.
2020/05/28 12:58:04 [emerg] 1#1: bind() to 0.0.0.0:80 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
2020/05/28 12:58:04 [emerg] 1#1: bind() to 0.0.0.0:80 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
2020/05/28 12:58:04 [emerg] 1#1: bind() to 0.0.0.0:80 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
2020/05/28 12:58:04 [emerg] 1#1: bind() to 0.0.0.0:80 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
2020/05/28 12:58:04 [emerg] 1#1: bind() to 0.0.0.0:80 failed (98: Address already in use)
nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use)
2020/05/28 12:58:04 [emerg] 1#1: still could not bind()
nginx: [emerg] still could not bind()

# 删除pod
[root@master 20200528]# kubectl delete pod myapp-pod
pod "myapp-pod" deleted

# 删除myapp.yaml中test容器

[root@master 20200528]# kubectl apply -f myapp.yaml 
pod/myapp-pod created

[root@master 20200528]# kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE    IP           NODE              NOMINATED NODE   READINESS GATES
myapp-pod   1/1     Running   0          111s   10.244.2.2   node02.alec.com   <none>           <none>
```

```
[root@node02 ~]# curl http://10.244.2.2
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
...
...
```

# 6. Pod

## 6.1 Pod概念

Pod是K8S系统中可以创建和管理的最小单元, 是资源对象模型中由用户创建或部署的最小资源对象模型, 也是在K8S上运行容器化应用的资源对象;

其他资源对象都是用来支撑或者扩展Pod对象功能的, 比如控制器对象用来管理Pod对象, Serivce或者Ingress资源对象用来暴露Pod引用对象, PersistentVolume资源对象用来为Pod提供存储等;

K8S不会直接处理容器, 而是通过Pod实现对容器的操作, Pod是由一个或多个container组成;

Pod是Kubernetes的最重要概念, 每一个Pod都有一个特殊的被称为Pause容器;

Pause容器对应的镜像属于Kubernetes平台的一部分, 除了Pause容器, 每个Pod还包含一个或多个紧密相关的用户业务容器;

**总结:**

- Pod是K8S中可以创建和管理的最小单元
- K8S不会直接处理容器, 而是通过Pod管理容器
- 一个Pod中, 会包含多个容器
- 一个Pod中的容器共享同一网络空间
- Pod是短暂的, 当Pod所在主机发生故障, Pod会被转移到其他主机

## 6.2 Pod存在的意义

- 使用Docker创建容器, 容器中运行进程, 一个容器对应一个应用
- Pod是多进程设计, 运行多个应用程序
    - 一个Pod有多个容器, 一个容器运行一个应用程序
- Pod存在是为了亲密性应用
    - 两个应用之间需要进行交互, 可以放在同一个Pod中
    - 网络之间的调用
    - 两个应用需要频繁调用

## 6.3 Pod实现机制

### 6.3.1 共享网络

Pod中首先创建Pause容器(根容器 or info容器); 随后创建业务容器, 比如 nginx容器, tomcat容器; 业务容器, 会被加入到Pause容器的网络中;

```
spec:
   container:
   - 容器列表
```

### 6.3.2 共享存储

Volume数据卷

## 6.4 Pod策略

### 6.4.1 镜像拉取策略

**spec.containers[].imagePullPolicy**  

镜像拉取策略, 有Always, Never, IfNotPresent三个值可选, 默认是Always

(1) Always: 每次构建都尝试重新拉取镜像

(2) Never: 表示仅适用本地镜像

(3) IfNotPresent: 如果本地有镜像就用本地镜像, 没有就拉取在线镜像

### 6.4.2 重启策略

**spec.restartPolicy**

Pod重启策略, 可选值为 Always、OnFailure、Never, 默认值为Always

(1) Always: Pod一旦终止运行, 则无论容器时如何终止的, kubelet服务都将重启它

(2) OnFailure: 只有Pod以非零退出码终止时, kubelet才会重启该容器; 如果容器正常退出(退出码为0), 则 kubelet 将不会重启它

(3) Never: Pod 终止后, kunelet 将退出码报告给 Master, 不会重启该 Pod

### 6.4.3 资源限制

**容器运行时资源的运行上限**

- spec.containers[].resource.limits.cpu	指定CPU的限制, 单位为 core 数, 将用于 docker run --cpu-share 参数
- spec.containers[].resource.memory  指定MEM内存的限制, 单位为MIB, GIB  

**容器启动和调度时的限制设置**(容器启动时, 要满足这个资源限制)

- spec.containers[].resource.requests.cpu  CPU请求, 单位为 core 数, 容器启动时初始化可用数量 
- spec.containers[].resource.requests.memory  内存请求, 单位为 MIB, GIB 容器启动的初始化可用数量

```
spec:
  containers:
  - 容器
    resource:
      limits:
        memory: "128Mi"
        cpu: "500m"
      requests:
        memory: :"64Mi"
        cpu: "250m"

Cpu: 1c 1000Mhz, 定义limits.cpu 500m, 说明这个容器最多可以使用0.5个CPU; 
```

## 6.5 健康检查

**livenessProbe** 存活检查

- 如果检查失败, 将杀死容器, 然后根据Pod的restartPolicy来操作容器重启;

**readinessProbe** 就绪检查

- 如果检查失败, Kubernetes会把Pod从service endpoints中剔除;

**Probe支持以下三种检查方法:**

- httpGet

    发送HTTP请求, 返回200-400范围状态码为成功

- exec

    执行shell命令, 返回状态码是0为成功

- tcpSocket

    发起TCP Socket, 建立连接则成功

## 6.6 调度策略

### 6.6.1 创建Pod流程

```
# Server端
kubectl(create pod) -请求-> apiserver -存储创建信息-> etcd
apiserver -监控创建请求-> scheduler
apiserver -保存pod调度信息-> etcd

# Node端
apiserver -创建Pod-> kubelet -创建容器-> Docker --kubelet--更新pod信息-> apiserver -保存Pod信息->
```

### 6.6.2 影响调度的属性

- Pod资源限制
- 节点标签选择器(nodeSelector)
- 节点亲和性(nodeAffinity)
- 污点和污点容忍

> Pod资源限制

根据requests找到满足Pod运行的节点

> 节点标签选择器(nodeSelector)

通过标签选择器选择Pod运行节点

```
# 给node set 标签
kubectl label node node-1 envrole=dev

# 查看标签
kubectl get nodes node-1 --show-labels
```

> 节点亲和性(nodeAffinity)

和nodeSelector类似, 根据节点标签来调度Pod到哪些节点

- 硬亲和性

    表示约束调钱必须满足

- 软亲和性

    表示尝试满足, 不满足也可以

    ```yaml
    spec:
      nodeAffinity:
        # 硬亲和性
        requiredDuringSchedulingIgnoreDuringExecution:
          nodeSelectorTerm:
          - matchExpressions:
            - key : env_role
              operator: In
              values:
              - dev
              - test
        # 软亲和性
        preferredDuringSchedulingIgnoreDuringExecution:
        - weight: 1
          preference:
            matchExceptions:
            - key: group
              operator: In
              values:
              - otherprod
    ```

    **operator 支持常用操作符**

    ```
    - In
    - NotIn
    - Exists
    - Gt
    - Lt
    - DoesNotExists
    ```

- 反亲和性

> 污点和污点容忍





## 6.7 Pod 生命周期

```shell
|                                    Pod 生命周期                                             |
| --容器初始化--> | init C          |             ---------------Liveness 阶段--------------- |
|                |          init C |             -readiness阶段-                  -STOP 时间- |
|                | ----------------------------------Pause Container------------------------ |
|                |                 | -------------------Main Container 运行时间-------------- |
|                |      		   | -start 时间-                                             |
```



- init Container：
    - 初始化容器，初始化之后自动清除；
    - 多个 init C 不能并行
    - 一个运行完成后,才能运行下一个
- Pause Container: 根容器
- Main Container：主容器
- readiness：就绪检测
- liveness：生存检测



### 6.7.1 init Container

Pod中能够具有多个容器, 应用运行在容器内, 但是Pod也可能有一个或多个先于应用容器启动的Init容器;

Init容器与普通容器非常像, 除了以下两点:

- Init容器总是运行到成功完成为止
- 每个Init容器都必须在下一个Init容器运行前完成

如果Pod的Init容器运行失败, Kubernetes会不断的重启该Pod, 直到Init容器成功为止;

然而如果Pod对应的restartPolicy是Never, 则Pod不会重启;



### 6.7.2 Init container 作用

- 因为 init容器 具有与应用程序容器分离的单独镜像, 所以它们的启动相关代码具有如下优势:

    - init容器 可以包含并运行使用工具, 但出于安全考虑, 是不建议在应用程序中包含这些实用工具的
    - init容器 可以包含使用工具和定制化代码来安装, 但是不能出现在应用程序镜像中; 例如, 创建镜像没必要FROM另一个镜像, 只需要在安装过程中使用类似 sed、awk、python或dig这样的工具；
    - 应用程序镜像可以分离出创建和部署的角色, 二没有必要联合 init容器 构建一个单独的镜像;
    - init容器  使用Linux Namespace, 所以相对应用程序来说, 具有不同的文件系统; 因此, 他们能够具有访问Secret的权限, 而应用程序则不能;
    - init容器 必须在应用程序驱动之前运行, 而应用程序容器时并行运行的, 所以Init容器能够提供了一种简单的阻塞或延迟应用容器启动的方法, 知道满足了一组先决条件;



> Demo Init容器

**init模板**

init-pod.yaml

```
apiVersion: v1
kind: Pod
metadata: 
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:  # main C
  - name: myapp-container
    image: busybox:latest
    command: ['sh', '-c', 'echo The app is running ! && sleep 3600']
  initContainers: # init Container
  - name: init-myservice
    image: busybox:latest
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:latest
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
    
[root@master 20200528]# kubectl create -f init-pod.yaml 
pod/myapp-pod created

[root@master 20200528]# kubectl get pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          27s

[root@master 20200528]# kubectl logs myapp-pod -c init-myservice
log is DEPRECATED and will be removed in a future version. Use logs instead.
;; connection timed out; no servers could be reached

waiting for myservice
;; connection timed out; no servers could be reached
...
...
```

myservice.yaml

```
kind: Service
apiVersion: v1
metadata:
  name: myservice
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376

[root@master 20200528]# kubectl create -f myservice.yaml
service/myservice created

[root@master 20200528]# kubectl get pod
NAME        READY   STATUS     RESTARTS   AGE
myapp-pod   0/1     Init:0/2   0          5m19s

[root@master 20200528]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   93d
myservice    ClusterIP   10.105.204.143   <none>        80/TCP    24s

```

mydb.yaml

```
kind: Service
apiVersion: v1
metadata:
  name: mydb
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9377

[root@master 20200528]# kubectl create -f mydb.yaml 
service/mydb created

[root@master 20200528]# kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1        <none>        443/TCP   101d
mydb         ClusterIP   10.101.247.93    <none>        80/TCP    33s
myservice    ClusterIP   10.105.204.143   <none>        80/TCP    8d

[root@master 20200528]# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          8d

[root@master 20200528]# kubectl exec -it myapp-pod -c myapp-container -- /bin/sh
```



> 特殊说明

- 在Pod启动过程中, Init容器会按顺序在网络和数据卷初始化之后启动; 每个容器必须在下一个容器启动之前成功退出;
- 如果由于运行时或失败退出, 将导致容器启动失败, 它会根据Pod的restartPolicy指定的次略进行重试;

- 在所有的Init容器没有成功之前, Pod将不会变成Ready状态了Init容器的端口将不会再Service 中进行聚集; 正在初始化中的Pod处于Pending状态, 但应该会将Initializaing状态设置为True;

- 如果重启Pod, 所有的Init容器必须重新执行;

- 对Init容器spec的修改被限制在image字段, 修改其他字段都不会生效; 更改init容器的image字段, 等价于重启该Pod;

- Init 容器具有应用容器的所有字段; 除了 readinessProbe, 因为 Init 容器无法定义不同于完成(competion)和就绪(readiness)之外的其他状态;

- 在Pod中的每个app和Init容器的名称必须唯一; 与任何其他容器共享同一个名称, 会在验证时抛出错误;



### 6.7.3 探针

探针是由kubelet对容器指定的定期诊断; 要执行诊断, kubelet调用由容器实现的Handler; 有三种类型的处理程序:

- ExecAction: 在容器内执行指定命令; 如果命令退出时返回码为0, 则认为陈宫;
- TCPSocketAction: 对指定端口上的容器的IP地址进行TCP检查; 如果端口打开, 则诊断被认为是成功的;
- HTTPGetAction: 对指定端口和路径上的容器的IP地址进行HTTP GET请求; 如果响应的状态大于等于200且小于400, 则认为诊断是成功的;

每次探测都将获得以下三种结果之一:

- 成功: 容器通过了诊断;
- 失败: 容器未通过诊断;
- 未知: 诊断失败, 因此不会采取任何行动;

> 探测方式

- livenessProbe
    - 指示容器是否正在运行;
    - 如果存货探测失败, 则kubelet会杀死容器, 并且容器将会受到其重启策略的影响;
    - 如果容器不提供存活探针, 则默认状态为Success;
- redinessProbe
    - 指示容器是否准备好服务请求;
    - 如果就绪探测失败, 端点控制器将从与Pod匹配的所有Service的端点中删去该Pod的IP地址; 
    - 初始延迟之前的就绪状态默认为Failure;
    - 如果容器不提供就绪探针, 则默认状态为Success;



> Demo 检测探针 - 就绪检测

```
apiVersion: v1
kind: Pod
metadata: 
  name: rediness-httpget-pod
  namespace: default
spec:
  containers: # main C
  - name: rediness-httpget-container
    image: nginx:1.14-alpine
    imagePullPolicy: IfNotPresent
    readinessProbe:
      httpGet:
        port: http
        path: /index1.html
      initialDelaySeconds: 1
      periodSeconds: 3
        
[root@master 20200606]# kubectl create -f rediness.yml 
pod/rediness-httpget-pod created

# Pod已经开始运行，但是还没有就绪；
[root@master 20200606]# kubectl get pods
NAME                   READY   STATUS    RESTARTS   AGE
rediness-httpget-pod   0/1     Running   0          16s

# describe 看到就绪检测未通过；
[root@master 20200606]# kubectl describe pod rediness-httpget-pod
......
  Normal   Started    2m5s                 kubelet, node02.alec.com  Started container
  Warning  Unhealthy  63s (x21 over 2m3s)  kubelet, node02.alec.com  Readiness probe failed: Get http://127.0.0.1:80/index1.html: dial tcp 127.0.0.1:80: connect: connection refused

# 手动进入Pod补全index1.html
[root@master 20200606]# kubectl exec rediness-httpget-pod -it -- /bin/sh
/ # cd /usr/share/nginx/html/
/usr/share/nginx/html # touch index1.html

[root@master 20200606]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
rediness-httpget-pod   1/1     Running   0          25m
```



> Demo 检测探针 - 存活探针

livenessProbe-exec

```
apiVersion: v1
kind: Pod
metadata: 
  name: liveness-exec-pod
  namespace: default
spec:
  containers: # main C
  - name: liveness-exec-container
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh", "-c", "touch /tmp/live; sleep 60; rm -f /tmp/live; sleep 3600"]
    livenessProbe:
      exec:
        command: ["test", "-e", "/tmp/live"]
      initialDelaySeconds: 1
      periodSeconds: 3

[root@master 20200606]# kubectl create -f liveness.yml 
pod/liveness-exec-pod created

# Pod running Ready；
[root@master 20200606]# kubectl get pod
NAME                READY   STATUS    RESTARTS   AGE
liveness-exec-pod   1/1     Running   0          12s

# 由于删除/tmp/live，100s后存活检测发现异常，重启Pod；
[root@master 20200606]# kubectl get pod
NAME                READY   STATUS    RESTARTS   AGE
liveness-exec-pod   1/1     Running   1          100s
```

livenessProbe-httpGet

```
apiVersion: v1
kind: Pod
metadata: 
  name: liveness-httpget-pod
  namespace: default
spec:
  containers: # main C
  - name: liveness-httpget-container
    image: nginx:1.14-alpine
    imagePullPolicy: IfNotPresent
    ports:
      - name: http
        containerPort: 80
    livenessProbe:
      httpGet:
        port: 80
        path: /index.html
      initialDelaySeconds: 1
      periodSeconds: 3
      timeoutSeconds: 10   # 每次访问超时时间
      
[root@master 20200606]# kubectl create -f liveness-httpget.yml 
pod/liveness-httpget-pod created

[root@master 20200606]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
liveness-httpget-pod   1/1     Running   0          20s

# 进入容器删除index.html
[root@master 20200606]# kubectl exec liveness-httpget-pod -c liveness-httpget-container -it -- /bin/sh
/ # cd /usr/share/nginx/html/
/usr/share/nginx/html # rm index.html

# 容器发生重启
[root@master 20200606]# kubectl get pod
NAME                   READY   STATUS    RESTARTS   AGE
liveness-httpget-pod   1/1     Running   1          2m19s

# describe看到日志
[root@master 20200606]# kubectl describe pod liveness-httpget-pod
......
  Normal   Started    2m (x2 over 4m15s)  kubelet, node02.alec.com  Started container
  Warning  Unhealthy  2m (x3 over 2m6s)   kubelet, node02.alec.com  Liveness probe failed: HTTP probe failed with statuscode: 404
  Normal   Killing    2m                  kubelet, node02.alec.com  Killing container with id docker://liveness-httpget-container:Container failed liveness probe.. Container will be killed and recreated.
```

livenessProbe-tcp

```
apiVersion: v1
kind: Pod
metadata: 
  name: liveness-tcp-pod
  namespace: default
spec:
  containers: # main C
  - name: liveness-tcp-container
    image: nginx:1.14-alpine
    livenessProbe:
      initialDelaySeconds: 1
      timeoutSeconds: 10
      tcpSocket:
        port: 8080
      periodSeconds: 3

[root@master 20200606]# kubectl create -f liveness-tcp.yml 
pod/liveness-tcp-pod created

# 由于liveness检测不到8080端口，所以Pod不断重启，重启两次后重建容器；
[root@master 20200606]# kubectl get pod -w
NAME               READY   STATUS    RESTARTS   AGE
liveness-tcp-pod   1/1     Running   0          2s
liveness-tcp-pod   1/1   Running   1     9s
liveness-tcp-pod   1/1   Running   2     18s
liveness-tcp-pod   0/1   CrashLoopBackOff   2     27s
liveness-tcp-pod   1/1   Running   3     42s

# 查看下日志
[root@master 20200606]# kubectl describe pod liveness-tcp-pod
......
Events:
  Type     Reason     Age                From                      Message
  ----     ------     ----               ----                      -------
  Normal   Scheduled  78s                default-scheduler         Successfully assigned default/liveness-tcp-pod to node01.alec.com
  Normal   Started    60s (x3 over 77s)  kubelet, node01.alec.com  Started container
  Warning  Unhealthy  52s (x9 over 76s)  kubelet, node01.alec.com  Liveness probe failed: dial tcp 10.244.1.15:8080: connect: connection refused
  Normal   Killing    52s (x3 over 70s)  kubelet, node01.alec.com  Killing container with id docker://liveness-tcp-container:Container failed liveness probe.. Container will be killed and recreated.
  Warning  BackOff    51s (x2 over 52s)  kubelet, node01.alec.com  Back-off restarting failed container
  Normal   Pulled     37s (x4 over 77s)  kubelet, node01.alec.com  Container image "nginx:1.14-alpine" already present on machine
  Normal   Created    37s (x4 over 77s)  kubelet, node01.alec.com  Created container
```



### 6.7.4 启动退出动作

```
apiVersion: v1
kind: Pod
metadata: 
  name: lifecycle-demo
spec:
  containers: # main C
  - name: lifecycle-demo-container
    image: nginx:1.14-alpine
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/shar/message"]
      preStop:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the preStop handler > /usr/shar/message"]

[root@master 20200606]# kubectl create -f lifecycle.yml 
pod/lifecycle-demo created


```



> Pod phase可能存在的值

- 挂起（Pending）
    - Pod已被Kubernetes系统接受, 但有一个或者多个容器镜像尚未被创建;
    - 等待时间包括调度Pod的时间和通过网络下载镜像的时间, 这可能需要花点时间;
- 运行中（Running）
    - 该Pod已经被绑定到了一个节点上, Pod中所有的容器都已被创建; 
    - 至少有一个容器正在运行, 或者整处于启动或重启状态;
- 成功（Succeeded）
    - Pod 中所有容器都被成功终止, 并且不会再重启;
- 失败（Failed）
    - Pod中的所有容器都已终止了, 并且至少有一个容器是因为失败终止;
    - 也就是说, 容器以非零状态退出或者被系统终止;
- 未知（Unknow）
    - 因为某些原因无法取得 Pod 状态, 通常是因为与Pod所在主机通信失败;



# 7. 控制器

## 7.1 控制器概念

Kubernetes 中内建了很多 controller(控制器), 这些相当于一个状态机, 用来控制 Pod 的具体状态和行为;

控制器类型:

- ReplicationController(淘汰) 和 ReplicaSet
- Deployment
- DaemonSet
- StateFulSet
- Job/CronJob
- Horizontal Pod Autoscaling

> ~~ReplicationController~~ 和 ReplicaSet

ReplicationController(RC) 用来确保容器应用的副本数十种保持在用户定义的副本数, 即如果有容器异常退出, 会自动创建新的Pod来替代; 如果异常多出来的容器也会自动回收;

在新版本的 Kubernetes 中建议使用 ReplicaSet 来取代 ReplicationController;

ReplicaSet 跟 ReplicationController 没有本质的不通, 只是名字不一样, 并且 ReplicaSet 支持集合式的 标签selector;

> Deployment

Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义(declarative)方法, 用来替代以前的 ReplicationController 来方便的管理应用; 

典型的应用场景:

- 定义 Deployment 来创建 Pod 和 ReplicaSet (Deployment --创建--> ReplicaSet --创建-->Pod)
- 滚动升级和回滚应用
- 扩容和缩容
- 暂停和继续 Deployment

```
命令式编程:
	侧重于如何实现程序, 可以理解为面向过程编程;
声明式编程:
	侧重于定义想要什么, 可以理解为面向对象编程;
```

> DaemonSet

DaemonSet 确保全部(或者一些) Node 上运行一个 Pod 的副本;

当有Node加入集群时, 也会为他们新增一个Pod;

当有 Node 从集群移除时, 这些 Pod 也会被回收;

删除 DaemonSet 将会删除它所创建的所有 Pod;

DaemonSet 的一些应用场景:

- 运行集群存储 daemon, 例如在每个Node上运行 glusterd、ceph
- 在每个 Node 上运行日志收集 daemon, 例如 fluentd、logstash
- 在每个 Node 上运行监控 daemon, 例如 Prometheus Node Exporter、collectd、Datadog代理、New Relic代理, 或 Ganglia gmond

> Job

Job 负责批处理任务, 仅执行一次的任务, 它保证批处理任务的一个或多个Pod成功结束；

> CronJob

Cron Job 管理基于时间的 Job, 即:

- 在给定时间点只运行一次
- 周期性地在给定时间点运行

使用前提条件:

- 当前使用的 Kubernetes 集群版本 >= 1.8;
- 对于线圈版本的集群, 版本<1.8, 启动 API Server 时, 通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API

典型用法如下:

- 在给定时间点调度 Job 运行
- 创建周期性运行的Job, 例如: 数据库备份、发送邮件

> StatefulSet

StatfulSet 作为 Controller 为 Pod 提供唯一的标识; 它可以保证部署和 scale 的顺序:

StatfulSet 是为了解决有状态服务的问题(对应Deployments和ReplicaSets是为无状态服务而设计);

应用场景如下:

- 稳定的持久化存储, 即 Pod 重新调度还是能访问到相同的持久化数据, 基于 PVC 来实现;
- 稳定的网络标志, 即 Pod 重新调度后其 PodName 和 HostName 不变, 基于 Headless Service (即没有Cluster IP 的Service)来实现
- 有序部署, 有序扩展, 即 Pod 是由顺序的, 在部署或扩展的时候要依据定义的顺序依次进行 (即从0到N-1, 在下一个 Pod 运行之前, 所有之前的 Pod 必须都是 Running 和 Ready 状态), 基于 init container 来实现;
- 有序收缩, 有序删除 (即从N-1到0)

> Horizontal Pod Autoscaling

应用的资源使用率通常有高峰和低谷, Horizontal Pod Autoscaling 自动调整 Service 中的 Pod 数量, 使 Pod 水平自动缩放, 提高集群整体资源利用率;

## 7.2 RC & RS & Deployment

RC (ReplicationController) 主要的左右就是用来确保容器应用的副本数始终保持在用户定义的副本数;

即如果有容器异常退出, 会自动创建新的 Pod 来替代; 而如果异常多出来的容器, 也会自动回收;



Kubernetes 官方建议使用 RS (ReplicaSet) 替代 RC 进行部署, RS 跟 RC 没有本质的不同, 只是名字不一样, 并且 RS 支持集合式的 标签selector; 

```
# 查看apiVersion值
[root@master-1 ~]# kubectl explain replicaset
KIND:     ReplicaSet
VERSION:  apps/v1
```

```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: my-frontend	# 定义RS的Name
spec:
  replicas: 3
  selector:
    matchLabels:	# RS监控Pod副本数的标签
      tier: my-frontend
  template:
    metadata:	# 给Pod绑定的标签
      labels:
        tier: my-frontend
    spec:
      containers:
      - name: php-redis
        image: nginx:1.20.0
        env:
        - name: GET_HOSTS_FROM
          value: dns
        ports:
        - containerPort: 80
```

>创建ReplicaSet

```
[root@master-1 ~]# kubectl create -f rs.yaml 
replicaset.apps/my-frontend created
```

> 查看ReplicaSet

```
[root@master-1 ~]# kubectl get rs
NAME          DESIRED   CURRENT   READY   AGE
my-frontend   3         3         3       30s
```

> 查看Pod

```
[root@master-1 ~]# kubectl get pods
NAME                READY   STATUS              RESTARTS   AGE
my-frontend-6vqvf   1/1     Running             0          49s
my-frontend-f9kkb   0/1     ContainerCreating   0          49s
my-frontend-vlzx9   0/1     ContainerCreating   0          49s
```

> 删除Pod, RS会生成新的Pod

```
[root@master-1 ~]# kubectl delete pod --all
pod "my-frontend-6vqvf" deleted
pod "my-frontend-f9kkb" deleted
pod "my-frontend-vlzx9" deleted

[root@master-1 ~]# kubectl get pods --show-labels
NAME                READY   STATUS    RESTARTS   AGE   LABELS
my-frontend-42hnw   1/1     Running   0          52s   tier=my-frontend
my-frontend-85prp   1/1     Running   0          52s   tier=my-frontend
my-frontend-zzxvt   1/1     Running   0          52s   tier=my-frontend
```

> 修改Pod的标签, rs会再生成一个新的pod

```
[root@master-1 ~]# kubectl label pod my-frontend-42hnw tier=your-frontend --overwrite=True
pod/my-frontend-42hnw labeled

[root@master-1 ~]# kubectl get pods --show-labels
NAME                READY   STATUS    RESTARTS   AGE     LABELS
my-frontend-42hnw   1/1     Running   0          2m43s   tier=your-frontend
my-frontend-85prp   1/1     Running   0          2m43s   tier=my-frontend
my-frontend-wcgh4   1/1     Running   0          15s     tier=my-frontend
my-frontend-zzxvt   1/1     Running   0          2m43s   tier=my-frontend

# ReplicaSet 会通过 spec.selector.matchLabels 中定义的标签, 监控 pod 数量;
```

> 删除rs, 被RS监控的Pod也会被删除

```
[root@master-1 ~]# kubectl delete rs my-frontend
replicaset.apps "my-frontend" deleted

[root@master-1 ~]# kubectl get pods --show-labels
NAME                READY   STATUS    RESTARTS   AGE     LABELS
my-frontend-42hnw   1/1     Running   0          5m50s   tier=your-frontend
```



#### 7.2.1 RS 与 Deployment 的关联

- Deployment 创建 ReplicaSet, ReplicaSet 创建 Pod;
- 滚动更新:
    - Deployment 会创建一个新的 ReplicaSet, 逐个替换掉旧 ReplicaSet 中的 Pod;

#### 7.2.2 Deployment

Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义(declarative)方法, 用来替代以前的 ReplicationController 来方便的管理应用; 

典型的应用场景:

- 定义 Deployment 来创建 Pod 和 ReplicaSet (Deployment --创建--> ReplicaSet --创建-->Pod)
- 滚动升级和回滚应用
- 扩容和缩容
- 暂停和继续 Deployment

> 部署一个Nginx应用

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
  	matchLabels:
	  app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16.0
        ports:
        - containerPort: 80
```

```
[root@master-1 ~]# kubectl create -f deployment.yaml --record
deployment.apps/nginx-deployment created
# --record参数可以记录命令, 以便于查看每次revision的变化
```

> 扩容

```
kubectl scale deployment nginx-deployment --replicas 5
```

> 如果集群支持 horizontal pod autoscaling, 还可以为Deployment设置自动扩展

```
kubectl autoscale deployment nginx-deployment --max=10 --min=5 --cpu-percent=80

# 查看hpa
kubectl get hpa

# 删除自动扩展
kubectl delete hpa mginx-deployment
```

> 镜像更新, 可以看到有两个rs

```
kubectl set image deployment/nginx-deployment nginx=nginx:1.17.0
```

> 编辑Deployment的yaml文件

```
kubectl edit deployment/nginx-deployment
```

> 回滚

```
kubectl rollout undo deployment/nginx-deployment
```

> 查看回滚状态

```
kubectl rollout status deployment/nginx-deployment
```

> 查看历史RS

```
[root@master-1 ~]# kubectl get rs
NAME                          DESIRED   CURRENT   READY   AGE
nginx-deployment-75c7f965d8   5         5         0       6s
nginx-deployment-75c7f965d8   0         0         0       2m20s
```

##### Deployment更新策略

Deployment 可以保证在升级时只有一定数量的 Pod 是 down的; 默认 它会确保至少有比期望的 Pod 数量少一个 是 Up 状态(最多一个不可用);

Deployment 同时也可以确保值创建出超过期望数量一定数量的 Pod; 默认 它会确保比期望的 Pod 数量多一个的 Pod 是 Up 的(最多一个 surge);

##### Rollover(多个rollout并行)

创建一个有5个 `nginx:1.19.0` replica 的 Deployment;

假设在Deployment运行中, 只有有3个 `nginx:1.19.0` 的 replica 创建出来的, 还有2个未创建时候, 就开始更新 Deployment 中 nginx 镜像到 `nginx:1.20.0`;

在这种情况下, Deployment 会立即杀掉已创建的3个 `nginx:1.19.0`, 并开始创建 `nginx:1.20.0`;

Deployment 不会等到所有5个 `nginx:1.19.0` 的 Pod 全部创建完成后 才开始更新镜像

##### 回退Deployment

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
        matchLabels:
          app: nginx
  revisionHistoryLimit: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16.0
        ports:
        - containerPort: 80
```

只要 Deployment 的 rollout 被出发就会创建一个 revision; 

也就是说当且仅当 Deployment 的 Pod tempalte (如 .spec.template) 被更改, 例如更新 template 中的 label 和容器镜像时, 就会创建一个新的 revision;

其他的更新, 比如扩容 Deployment 不会创建 revision;

因此我们可以很方便的手动或者自动扩容; 这意味着当回退到历史的 Deployment 时, 只有 Deployment 中的 Pod template 部分才会回退;

```
# 升级Deployment中nginx镜像到1.17.0
kubectl set image deployment/nginx-deployment nginx=nginx:1.17.0

# 查看Deployment回滚状态, 如果 rollout 成功完成, kubectl rollout status 将返回一个 0 值的 exit code
kubectl rollout status deployment/nginx-deployment

# 查看历史版本, 加 --record 后, 就会在 CHANGE-CAUSE 显示更新的命令(并不推荐, 最好还是记录在变更记录中)
kubectl rollout history deployment/nginx-deployment

# 回滚Deployment
kubectl rollout undo deployment/nginx-deployment

# 回滚到指定版本Deployment
kubectl rollout undo deployment/nginx-deployment --to-revision=2

# 暂停Deployment回滚动作
kubectl rollout pause deployment/nginx-deployment
```

> 回退版本数记录

可以通过设置 `.spec.revisionHistoryLimit` 项来指定deployment最多保留多少 revision 历史记录;

默认会保留所有的 revision; 

如果该项设置为0, Deployment就无法回退了;



## 7.3 DaemonSet

DaemonSet 确保全部(或者一些) Node 上运行一个 Pod 的副本;

当有Node加入集群时, 也会为他们新增一个Pod;

当有 Node 从集群移除时, 这些 Pod 也会被回收;

删除 DaemonSet 将会删除它所创建的所有 Pod;

DaemonSet 的一些应用场景:

- 运行集群存储 daemon, 例如在每个Node上运行 glusterd、ceph
- 在每个 Node 上运行日志收集 daemon, 例如 fluentd、logstash
- 在每个 Node 上运行监控 daemon, 例如 Prometheus Node Exporter、collectd、Datadog代理、New Relic代理, 或 Ganglia gmond

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: daemonset-example
  labels:
    app: daemonset
spec:
  selector:
    matchLabels:
      labels:
        name: daemonset-example
  template:
    metadata:
      labels:
        name: daemonset-example
    spec:
      containers:
      - name: daemonset-example
        image: nginx:1.16.0
```



## 7.4 Job & CronJob

### 7.4.1 Job

Job 负责批处理任务, 仅执行一次的任务, 它保证批处理任务的一个或多个 Pod 成功结束;

**特殊说明:**

- spec.template 格式同 Pod
- restartPolicy 仅支持 Never 或 OnFailure
- 单个 Pod 时, 默认 Pod 运行成功后 Job 即结束
- .spec.completions 标志Job运行结束需要成功运行的 Pod 个数, 默认为 1
- .spec.parallelism 标志并行运行的 Pod 个数, 默认为1
- spec.activeDeadlineSeconds 标志失败 Pod 的最大重试最大时间, 超过这个时间不会继续重试

```
apiVersion: batch/v1
kind: Job
metadata:
  name: date
spec:
  template:
    metadata:
      name: date
    spec:
      containers:
      - name: date
        image: centos
        command: ["date"]
      restartPolicy: Never
```

```
[root@master-1 ~]# kubectl get job
NAME   COMPLETIONS   DURATION   AGE
date   1/1           34s        2m53s

[root@master-1 ~]# kubectl get pod
NAME                                READY   STATUS      RESTARTS   AGE
date-msksz                          0/1     Completed   0          3m2s

[root@master-1 ~]# kubectl logs date-msksz
Wed Jun  2 14:44:49 UTC 2021
```



### 7.4.2 CronJob

Cron Job 管理基于时间的 Job, 即:

- 在给定时间点只运行一次
- 周期性地在给定时间点运行

使用前提条件:

- 当前使用的 Kubernetes 集群版本 >= 1.8;
- 对于线圈版本的集群, 版本<1.8, 启动 API Server 时, 通过传递选项 --runtime-config=batch/v2alpha1=true 可以开启 batch/v2alpha1 API

典型用法如下:

- 在给定时间点调度 Job 运行
- 创建周期性运行的Job, 例如: 数据库备份、发送邮件

**特殊说明:**

- .spec.schedule: 调度, 必须字段, 指定任务运行周期, 格式同 `crontab`

- .spec.jobTemplate: Job 模板, 必须字段, 指定需要运行的任务, 格式同 Job

- .spec.startingDeadlineSeconds: 启动 Job 的期限(秒级别), 该字段是可选的;

  如果因为任何原因而错过了被调度的时间, 那么错过执行时间的 Job 将被认为是失败的;

  如果没有指定, 则没有期限;

- .spec.concurrencyPolicy: 并发策略, 该字段是可选的;

  该字段指定了如何处理被 CronJob 创建的 Job的并发执行, 只允许下面策略的一种:

  - Allow 默认: 允许并发运行 Job
  - Forbid: 禁止并发运行, 如果前一个还没有完成, 则直接跳过下一个
  - Replace: 取消当前正在运行的 Job, 用一个新的来替换

  **注意**, 当前策略只能应用于同一个 CronJob创建的 Job; 如果存在多个 CronJob, 他们创建的 Job 之间总是运行并发运行;

- .spec.suspend: 挂起, 该字段可选; 默认为 false;

  如果设置为 true, 后续所有执行都会被挂起;

  该字段对已经开始执行的 Job 不起作用

- .spec.successfulobsHistoryLimit 和 .spec.fauledJobsHistoryLimit: 历史限制, 可选字段; 

  指定了可以保留多少完成和失败的 Job;

  默认值分别是 3 和 1; 

  设置为0, 相关类型的 Job 完成后将不会被保留;

```
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "* * * * *"
  jobTemplate:
    spec:
      template:
        spec:
         containers:
         - name: hello
           image: centos
           args:
           - /bin/bash
           - -c
           - date; echo hello
         restartPolicy: OnFailure
```

```
[root@master-1 ~]# kubectl get cronjob
NAME    SCHEDULE    SUSPEND   ACTIVE   LAST SCHEDULE   AGE
hello   * * * * *   False     1        14s             106s

[root@master-1 ~]# kubectl get jobs
NAME               COMPLETIONS   DURATION   AGE
hello-1622645760   1/1           17s        88s
hello-1622645820   1/1           18s        28s

[root@master-1 ~]# kubectl get pods
NAME                     READY   STATUS      RESTARTS   AGE
hello-1622645760-ggjkw   0/1     Completed   0          97s
hello-1622645820-6x9j7   0/1     Completed   0          37s

[root@master-1 ~]# kubectl logs hello-1622645760-ggjkw
Wed Jun  2 14:56:24 UTC 2021
hello

# 注意删除cronjob 的时候 可能 不会自动删除 job; 这些 job 可以用 kubectl delete job 来删除
```

> CronJob自身限制

创建 Job操作应该是 幂等的

