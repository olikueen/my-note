[toc]

# 8. Service

## 8.1 Service 概念

Kubernetes Service 定义了一种抽象: 一个 Pod 的逻辑分组, 一种可以访问他们的策略 ------ 通常称为微服务;

这一组 Pod 能够被 Service 访问到, 通常是通过 Label Selector;

Service能够提供负载均衡的能力, 但是只能提供 4 层负载均衡能力, 而没有 7 层功能;

### 8.1.1 Service的类型

Service 在 k8s  中有以下 4种 类型:

- ClusterIp: 默认类型, 自动分配一个仅 Cluster 内部可以访问的虚拟 IP
- NodePort: 在 ClusterIP 基础上为 Service 在每台 Node 机器上绑定一个端口, 这样就可以通过 `NodeIP:NodePort` 来访问该服务
- LoadBalancer: 在 NodePort 的基础上, 借助 cloud provider 创建一个外部负载均衡器, 并将请求转发到 `NodeIP:NodePort`
- ExternalName: 把集群外部的服务引入到集群内部来, 在集群内部直接使用; 没有任何类型代理被创建, 只有 kubernetes 1.7 或更高版本的 kube-dns 才支持

### 8.1.2 VIP 和 Service 代理

在 Kubernetes 集群转中, 每个 Node 运行一个 kube-proxy 进程;

kube-proxy 负责为 Service 实现了一种 VIP (虚拟IP) 的形式, 而不是 ExternalName 的形式;

在 Kubernetes v1.0 版本, 代理完全在 userspace;

在 k8s v1.1 版本, 新增了 iptables 代理, 单并不是默认的运行模式; 从 k8s v1.2 起, 默认就是 iptables代理; 在 k8s v1.8.0-beta.0 中, 添加了 ipvs 代理; 在 k8s v1.14 版本默认使用 ipvs代理;

在 k8s v1.0 版本, Service 是 4层 代理; 在 kubernetes v1.1 版本, 新增了 Ingress API, 用来表示 7层 代理;

### 8.1.3 代理模式分类

- userspace代理模式

- iptables代理模式

- ipvs代理模式

  这种模式, kube-proxy 会监视 Kubernetes Service 对象和 Endpoints, 调用 netlink 接口以相应的创建 ipvs 规则, 并定期与 Kubernets Service 对象和 Endpoints 对象同步 ipvs 规则, 以确保 ipvs状态与期望一致; 访问服务是, 流量将被重定向到其中一个后端 Pod;

  与 iptables类似, ipvs 与 netfilter 的 hook 功能, 但使用哈希表作为底层数据结构并在内核空间中工作; 这表示 ipvs 可以更快的重定向流量, 并且在同步代理规则时具有更好的性能;

ipvs支持的负载均衡算法:

- rr: 轮询调度
- lc: 最小连接数
- dh: 目标哈希
- sh: 源哈希
- sed: 最短期望延迟
- nq: 不排队调度

```
注意:
ipvs 模式假定 在运行 kube-proxy 之前, 已经在节点上都已经安装了 IPVS 内核模块; 
当 kube-proxy 以 ipv 代理模式启动时, kube-proxy 将验证节点上是否安装了 IPVS 模块;
如果未安装, 则 kube-proxy 将回退到 iptables 代理模式;
```



## 8.2 Service类型

### 8.2.1 Cluster IP

ClusterIP 会给 Service 分配一个**仅 Cluster 内部**可以访问的虚拟 IP;

clusterIP 主要在每个 node 节点使用 iptables, 将发向 cluster对应端口的数据, 转发到 kube-proxy ;

然后 kube-proxy 自己内部实现有负载均衡的方法, 并可以查询到这个 service 下对应 pod 的地址和端口, 进而把数据转发给对应的 pod 地址和端口

```
Pod(Forntend)      | Service              | Pods(Backend)
-------------------|----------------------|----------------
                   |                      | ┌-- webApp-1
                   |                      | │
Nginx ---------------ClusterIP, Port -------┼-- webApp-2
                   |                      | │
                   |                      | └-- webApp-3
```

为了实现上图的功能, 主要需要以下几个组件的协同工作:

- apiserver: 用户通过 kubectl 命令向 apiserver 发送创建 service 请求, apiserver 接收到请求后, 将数据存储到 etcd 中;
- kube-proxy: k8s 的每个节点的 kube-proxy进程负责感知 service、pod 的变化, 并将变化的信息写入到本地的 iptables 规则中
- iptables: 使用 NAT 等技术将 virtualIP 的流量转至 endpoint 中

> 创建Pod

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deploy
  namespace: default
spec:
  replicas: 3
  selector:
    matchLabels:
      app: myapp
      release: stabel
  template:
    metadata:
      labels:
        app: myapp
        release: stabel
        env: test
    spec:
      containers:
      - name: myapp
        image: nginx:1.16.0
        imagePullPolicy: IfNotPresent
```

> 创建Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: myapp		# service name
  namespace: default
spec:
  type: ClusterIP	# service 后端绑定的 Pod 的标签
  selector:
    app: myapp
    release: stabel
  ports:
  - name: http
    port: 80
    targetPort: 80
```

### 8.2.2 Headless Service

有时不需要或不想要负载均衡, 以及单独的ServiceIP, 可以通过指定 ClusterIP(spec.clusterIP) 的值为 "None" 来创建 Headless Service;

这类 Service 并不会分配 Cluster IP, kube-proxy不会处理它们, 而且平台也不会为他们进行负载均衡和路由;

```
apiVersion: v1
kind: Service
metadata:
  name: myapp-headless
  namespace: default
spec:
  selector:
    app: myapp
  clusterIP: "None"
  ports:
  - port: 80
    targetPort: 80
```

svc 一旦创建, 会被写入 coredns, 格式: `SVC_NAME.NAMESPACE.svc.cluster.local`

```
# 查看coredns IP
kubectl get pod -n kube-system -o wide
dig -t A myapp-headless.default.svc.cluster.local. @10.244.0.46
```



### 8.2.3 NodePort

nodePort 的原理在于在 node上开了一个端口, 将向该端口的流量导入到 kube-proxy, 然后由 kube-proxy 进一步给到对应的 pod;

```
apiVersion: v1
kind: Service
metadata:
  name: myapp
  namespace: default
spec:
  type: NodePort
  selector:
    app: myapp
    release: stabel
  ports:
  - name: http
    port: 80		// 集群内访问
    nodePort: 80	// 集群外访问
    targetPort: 80	// j'k
```

查询端口映射信息

```
iptables -t nat -nvL
	KUBE-NPDEPORTS
```

### 8.2.4 LoadBalancer

loadBalancer 和 nodePort 其实是同一种方式; 区别在于 loadBalancer 比 nodePort 多了一步, 就是可以调用 cloud provider 去创建 LB 来向节点导流;



### 8.2.4 ExternalName

这种类型的 Service 通过返回 CNAME 和它的值, 可以将服务映射到 externalName 字段的内容;

ExternalName Service 是 Service 的特例, 它没有 selector, 也没有定义任何的端口和 Endpoint; 相反的, 对于运行在集群外部的服务, 它通过返回该外部服务的别名, 这种方式来提供服务;

```
apiVersion: v1
kind: Service
metadata:
  name: my-service-1
  namespace: default
spec:
  type: ExtrtnalName
  externalName: hub.atguigu.com
```

当查询主机 my-service.default.svc.cluster.local (SVC_NAME.NAMESPACE.svc.cluster.lcoal) 时, 集群的DNS服务将返回一个值 my.database.example.com 的 CNAME 记录; 访问这个服务的工作方式 和 其他的相同, 唯一不同的是重定向到发生在 DNS 层, 而且不会进行代理或转发;

## 8.3 Ingress

### 8.3.1 部署Ingress-Nginx

```shell
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

helm pull ingress-nginx/ingress-nginx --untar

cd ingress-nginx
vim values.yaml

# 修改以下内容
 10 controller:
 11   name: controller
 12   image:
 13     registry: registry.aliyuncs.com/google_containers	# 修改registry地址, 国内k8s.gcr.io访问不到
 14     image: ingress-nginx/controller
...
 18     tag: "v0.47.0"
 19     #digest: sha256:a1e4efc107be0bb78f32eaec37bef17d7a0c81bec8066cdf2572508d21351d0b	# 注释掉镜像校验
 20     pullPolicy: IfNotPresent
...
 55   dnsPolicy: ClusterFirstWithHostNet	# 和64行关联, 如果hostNetwork不为true, 会导致ingress无法解析pod域名
...
 64   hostNetwork: true
...
165   kind: DaemonSet	# DaemonSet可以把ingress部署到指定的节点
...
263   nodeSelector:
264     kubernetes.io/os: linux
265     ingress: "true"		# 让ingress只能部署在有 ingress:"true" 标签的节点
...
324     requests:			# 节点资源限制, 按需配置
325       cpu: 100m
326       memory: 90Mi
...
437     type: ClusterIP		# 云环境才可以使用LoadBalancer, 本地使用ClusterIP
...
507   admissionWebhooks:	# 准入控制
508     annotations: {}
509     enabled: true		# 低版本改成false
...
653   image:
654     registry: registry.aliyuncs.com/google_containers
655     image: defaultbackend-amd64
```

```shell
kubectl create namespace ingress-nginx
helm install ingress-nginx . -n ingress-nginx
```



### 8.3.2 Ingress HTTP 代理访问

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-dm
spec:
  relicas: 2
  template:
    labels:
      name: nginx
  spec:
    containers:
    - name: nginx
      image: nginx:1.16.0
      imagePullPolicy: IfNotPresent
      ports:
      - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  namge: nginx-svc
spec:
  ports:
  - port: 80
    targetPort: 80
    protocol: tcp
  selector:
    name: nginx
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-test
spec:
  rules:
  - host: www.alec.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx-svc
          servicePort: 80
```

### 8.3.3 Ingress HTTPS代理访问

创建证书

```
ORGANIZE='alec'
DOMAIN='www.alec.com'

openssl req -new -SHA256 -newkey rsa:2048 -nodes -keyout $DOMAIN.key -out $DOMAIN.csr -subj "/C=CN/ST=BJ/L=BJ/O=$ORGANIZE/OU=/CN=$DOMAIN"

openssl x509 -req -in $DOMAIN.csr -signkey $DOMAIN.key -days 36500 -out $DOMAIN.pem
```

Ingress

```

```

### 8.3.4 nginx url重写

| 名称                                           | 描述                                                         | 值     |
| ---------------------------------------------- | ------------------------------------------------------------ | ------ |
| nignx.ingress.kuberentes.io/rewrite-target     | 必须重定向的目标URI                                          | string |
| nginx.ingress.kubernetes.io/ssl-redirect       | 指示位置部分是否仅可访问SSL(当Ingress包含证书时, 默认为True) | bool   |
| nginx.ingress.kubernetes.io/force-ssl-redirect | 即使Ingress未启用TLS, 也强制重定向到HTTPS                    | bool   |
| nginx.ingress.kubernetes.io/app-root           | 定义Controller必须重定向的应用程序根, 如果它在'/'上下文中    | string |
| nginx.ingress.kubernetes.io/use-regex          | 指示Ingress上定义的路径是否使用正则表达式                    | bool   |

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx-test
  annotations:
    nignx.ingress.kuberentes.io/rewrite-target: http://foo.bar.com:31795/hostname.html
spec:
  rulles:
  - host: foo10.bar.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx-svc
          servicePort: 80
```



# 9. 存储

## 9.1 configMap

ConfigMap 功能 Kubernetes1.2 版本中引入, 许多应用程序会从配置文件、命令行参数或环境变量中读取配置信息;

ConfigMap API 提供了向容器中配置信息的机制, ConfigMap 可以被用来保存单个属性, 也可以用来保存整个配置文件或者JSON二进制大对象;

### 9.1.1 configMap创建

#### 使用目录创建

```
ls conf/games/configmap
game.properties
ui.properties

cat game.properties
enemies=aliens
lives=3
enemies.cheat=true

cat ui.properties
color.good=purple
color.pad=yellow
allow.textmod=true

kubectl create configmap game-config --from-file=conf/games/configmap
```

--from-file 在指定目录下的所有文件都会被在 ConfigMap 里创建一个键值对, 键的名字就是文件名, 值就是文件内容

#### 使用文件创建

--from-file 指向一个文件

--from-file 可以在一条命令内多次使用

```
kubectl create configmap db-config --from-file=db.properties

[root@master-1 ~]# kubectl get configmap db-config -o yaml
apiVersion: v1
data:
  db.properties: |
    hostname=127.0.0.1
    port=3306
    user=root
    password=admin@123
    name=test
kind: ConfigMap
......
```

#### 使用字符串创建

通过 --from-literal 传递配置信息, 可以在一条命令内多次使用

```
kubectl create donfigmap special-config --from-literal=special.how=very --from-literal=special.type=charm

kubectl get configmap special-config -o yaml
```

### 9.1.2 Pod中使用ConfigMap

#### 使用ConfigMap代替环境变量

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
```

```
apiVersion: v1
kind: COnfigMap
metadata:
  name: env-config
  namespace: default
data:
  log_level: INFO
```

```
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
  - name: test-conainer
    image: nginx:1.16.0
    command:
    - /bin/bash
    - -c
    - env
    env:
    - name: SPECIAL_LEVEL_KEY
      valueFrom:
        configMapKeyRef:
          name: special-config
          key: special.how
    - name: SPECIAL_TYPE_KEY
      valueFrom:
        configMapKeyRef:
          name: special-config
          key: special.type
    envFrom:
    - configMapRef:
        name: env-config
  restartPolicy: Never
```

#### 用ConfigMap设置命令行参数

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
```

```
apiVersion: v1
kind: Pod
meadata:
  name: dapi-test-pod
spec:
  containers:
  - name: test-container
    image: nginx:1.16.0
    command:
    - /bin/sh
    - -c
    - echo $SPECIAL_LEVE_KEY $SPECIAL_TYPE_KEY
    env:
    - name: SPECIAL_LEVEL_KEY
      valueFrom:
        configMapKeyRef:
          name: special-config
          key: spcial.how
    - name: SPECIAL_TYPE_KEY
      valueFrom:
        configMapKeyRef:
          name: special-config
          key: special.type
  restartPolicy: Never

```

#### 通过数据卷插件使用ConfigMap

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: special-config
  namespace: default
data:
  special.how: very
  special.type: charm
```

在数据卷里使用ConfigMap, 有不同的选项; 

最基本的就是将文件填入数据卷, 在这个文件中, 键就是文件名, 键值就是文件内容;

```
apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
  - name: test-container
    image: nginx:1.16.0
    command: ["/bin/sh", "-c", "cat /etc/config/config/special.how"]
    volumeMounts:
    - name: config-volume
      configMap:
        name: special-config
  restartPolicy: Never
```

## 9.1.3 ConfigMap热更新

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: log-config
  namespace: default
data:
  log_level: INFO
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: my-nginx
spec:
  replicas: 1
  template:
    metadata:
      labels:
        run: my-nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.16.0
        ports:
        - containerPort: 80
        volumeMount:
        - name: config-volume
          mountPath: /etc/config
       valumes:
       - name: config-volume
         configMap:
           name: logconfig
```

```
kubectl exec `kubectl get pods -l run=my-nginx -o=name|cut -d "/" -f2` cat /etc/config/log_level
```

> 修改ConfigMap

```
kubectl edit configmap log-config
```

> 修改 log_level 的值为 DEBUG, 等待大约 10 秒钟时间, 再次查看环境变量的值

```
kubectl exec `kubectl get pods -l run=my-nginx -o=name|cut -d "/" -f2` cat /etc/config/log_level
```

> ConfigMap更新后滚动更新Pod

```
kubectl patch deployment my-nginx --patch '{"spec": {"template": {"metadata": {"annotations": {"version/config": "20190411"}}}}}'
```

这个例子, 在 .spec.template.metadata.annotations 中添加 version/config, 每次通过修改 version/config 来触发滚动更新;

**注意:** 更新ConfigMap后

- 使用该ConfigMap挂载的Env不会同步更新
- 使用该ConfigMap挂载的Volume中的数据需要一段时间(实测大概10秒)才能同步更新

## 9.2 Secret

### 9.2.1 Secret存在意义

Secret 解决了密码、token、秘钥等敏感数据的配置问题, 而不需要把这些敏感数据暴露到镜像或者 Pod Spec中;

Secret 可以 以 Volume 或者环境变量的方式使用;

**Secret 有三种类型:**

- Service Account: 用来访问Kubernetes API, 由Kubernetes自动创建, 并且会自动挂载到 Pod 的 `/run/secrets/kubetentes.io/serviceaccount` 目录中
- Opaque: base64 编码格式的 Secret, 用来存储密码、秘钥等
- kubernetes.io/dockerconfigjson: 用来存储私有 docker registry 的认证信息

### 9.2.2 Service Account

Service Account 用来访问 Kuberentes API, 由 Kubernetes 自动创建, 并且会自动挂载到 Pod 的 `/run/secrets/kubetentes.io/serviceaccount` 目录中

```
kubectl run nginx --image nginx:1.16.0

kubectl get pods

kubectl exec NGINX_POD_NAME ls /run/secrets/kuberntes.io/serviceaccount
```

### 9.2.3 Opaque Secret

#### 创建Secret

Opqaue 类型的数据是一个 map 类型, 要求 value 是 base64 编码格式;

```
[root@master-1 ~]# echo -n "admin" | base64
YWRtaW4=
[root@master-1 ~]# echo -n "1qaz2wsx3edc4rfv" | base64
MXFhejJ3c3gzZWRjNHJmdg==
```

```
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  usermane: YWRtaW4=
  password: MXFhejJ3c3gzZWRjNHJmdg==
```

#### 将Secret挂载到Volume

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    name: secret-test
  spec:
    volumes:
    - name: secrets
      secret:
        secretName: mysecret
    containers:
    - name: db
      image: nginx:1.16.0
      volumeMount:
      - name: secrets
        mountPath: "/mnt"
        readOnly: True
```

#### 将Secret导出到环境变量

```
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: pod-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: pod-deployment
    spec:
      containers:
      - name: pod-1
        image: nginx:1.16.0
        ports:
        - containerPort: 80
        env:
        - name: TEST_USER
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: username
        - name: TEST_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysecret
              key: password
```

### 9.2.4 kubernetes.io/dockerconfigjson

使用 Kubectl 创建 docker registry 认证的 secret

```
kubectl create secret docker-registry myregistrykey \
--docker-server=DOCKER_REGISTRY_SERVER \
--docker-username=DOCKER_USER \
--docker-password=DOCKER_PASSWORD \
--docker-email=DOCKER_EMAIL
```

在创建 Pod 的时候, 通过 imagePullSecrets 来引用创建的 myregistryjson

```
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
  - name: foo
    image: nginx:1.16.0
  imagePullSecrets:
  - name: myregistrykey
```

## 9.3 Volume

容器内磁盘上的文件的生命周期是短暂的, 这就使得在容器中运行中药应用时会出现一些问题;

首先, 当容器崩溃时, kubelet 会重启容器, 但是容器中的文件会丢失------容器以干净的状态运行(镜像最初的状态)重新启动;

其次, 在 Pod 中同时运行多个容器时, 这些容器之间通常需要共享文件;

Kubernetes 中的 volume 抽象就很好的解决了这些问题;

### 9.3.1 卷的类型

Kubernetes volume 支持以下类型的卷:

- awsElasticBlockStore azureDisk azureFile cephfs csi downwardAPI emptyDir
- fc flocker gcePersistentDisk gitRepo glusterfs hostPath iscsi local nfs
- persistenVolumeClaim projected portworxVolume quobyte rbd scaleIO secret
- storageos vsphereVolume

### 9.3.2 emptyDir

当 Pod 被分配给节点时, 首先创建 emptyDir 卷, 并且只要该Pod 在该节点上运行, 该卷就会存在;

正如卷的名字, emptyDir最初是空的;

Pod 中的容器可以读取和写入 emptyDir 卷中的相同该文件, 尽管该卷可以挂在到每个容器中的相同或不相同路径上;

当出于任何原因从节点中删除 Pod 时, emptyDir 中的数据将被永久删除;

emptyDir 用法:

- 暂存空间, 例如用于基于磁盘的合理排序
- 用作长时间计算崩溃恢复时的检查点
- Web服务器容器提供数据时, 保存内容管理器容器提取的文件

```
apiVerison: v1
kind: Pod
metaData:
  name: test-pd
spec:
  containers:
  - name: test-container
    iamge: nginx:1.16.0
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volume:
  - name: cache-volume
    emptyDir: {}
```

### 9.3.3 hostPath

hostPath卷 将主机节点的为文件系统中的文件或目录挂载到集群中;

hostPath用途:

- 运行需要访问Docker内部的容器; 使用 /var/lib/docker 的 hostPath
- 在容器中运行 cAdvisor; 使用 /dev/cgroups 的 hostPath

除了所需的path属性, 之外还可以为 hostPath 卷指定 type:

| 值                | 行为                                                         |
| ----------------- | ------------------------------------------------------------ |
|                   | 空字符串(默认)用于向后兼容, 这意味着在挂载 hostPath 卷之前不会执行任何检查 |
| DirectoryOrCreate | 如果在给定的路径上没有任何东西存在, 那么将根据需要在给定路径上创建一个空目录, 权限默认为0755, 与 kubelet 具有相同的组和所有权 |
| Directory         | 给定的路径下必须存在目录                                     |
| FileOrCreate      | 如果给定的路径上没有任何东西存在, 那么会根据需要创建一个空文件, 权限默认为0644, 与 kubelet 具有相同的组和所有权 |
| File              | 给定的路径下必须存在文件                                     |
| Socket            | 给定的路径下必须存在UNIX套接字                               |
| CharDevice        | 给定的路径下必须存在字符设备                                 |
| BlockDevice       | 给定的路径下必须存在块设备                                   |

**注意:**

- 由于每个节点上的文件都不同, 具有相同配置的pod在不同节点上的行为可能会有所不同
- 当Kubernetes按照计划添加资源感知调度时, 将无法考虑 hostPath 使用的资源

- 在底层主机上床架你的文件或目录只能由root写入; 需要在特权容器中以root身份运行进程, 或修改主机上的文件权限以便写入 hostPath 卷

```
apiVerison: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - name: test-container
    image: nginx:1.16.0
    volumeMounts
    - name: test-volume
      mountPath: /test-pd
  volumes:
  - name: test-volume
    hostPath:
      path: /data
      type: DirectoryOrCreate
```

### 9.3.4 Persistent Volume

**PersistentVolume(PV)**

- 是由管理员设置的存储, 它是集群的一部分;
- 就像节点是集群中的资源一样, PV 也是集群中的资源;
- PV是Volume之类的卷插件, 但具有独立于使用 PV 的 Pod 的生命周期;
- 此 API 对象包含存储实现的细节, 即NFS、ISCSI或特定云供应商的存储系统;

**PersistentVolumeClaim(PVC)**

- 是用户存储的请求
- 与 Pod 相似, Pod 消耗节点资源, PVC 消耗 PV 资源;
- Pod可以请求特定级别的资源(CPU和内存)
- 声明可以请求特定的大小和访问模式(例如, 可以读/写一次或只读多次模式挂载)

**静态pv**

- 集群管理员创建一些PV
- 带有可供集群用户使用的实际存储的细节
- 存在于Kubernetes API中, 可用于消费

**动态PV**

当管理员创建的静态PV都不匹配用户的 PersistentVolumeClaim 时, 集群可能会尝试动态地为 PVC 创建卷; 

此配置基于 StorageClasses: PVC必须请求存储类, 并且管理员必须创建并配置该类才能进行动态创建; 声明该类为 "" 可以有效地禁止动态配置;

要启用基于存储级别的动态存储配置, 集群管理员要启用 API Server 上的 DefaultStorageClass [准入控制器]; 

例如通过确保 DefaultStorageClass 位于 APIServer组件的 --admission-control 标志, 使用逗号分隔的有序值列表中, 可以完成操作;

#### 持久化卷类型

PersistentVolume 类型以插件形式实现; 目前支持一下插件类型

- GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC(Fibre Channel)
- FlexVolume Flocker NFS iScSI RBD(Ceph Block Device) CephFS
- Cinder(OpenStack Block Storage) Clusterfs VsphereVolume Quobyte Volimes
- HostPath VMWare Photon Portworx Volumes ScaleIO Volumes StorageOS



```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 5Gi
  volumeMode: filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
  - hard
  - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```



#### PV 访问模式

PersistentVolume 可以以资源提供者支持的任何方式挂载到主机上;

如下表所示, 供应商具有不同的功能, 每个 PV 的访问模式都将被设置为该卷支持的特定模式;

例如, NFS 可以支持多个读/写客户端, 但特定的 NFS PV 可能以只读方式导出到服务器上;

每个 PV 都有一套自己的用来描述特定功能的访问模式;

- ReadWriteOnce (**RWO**) ---- 该卷可以被单个节点以读/写模式挂载
- ReadOnluMany (**ROX**) ---- 该卷可以被多个节点以只读模式挂载
- ReadWriteMany (**RWX**) ---- 该卷可以被多个节点以读/写模式挂载

|      Volume插件      | ReadWriteOnce | ReadOnlyMant |    ReadWriteMany    |
| :------------------: | :-----------: | :----------: | :-----------------: |
| AwsElasticBlockStore |       √       |              |                     |
|      AzureFile       |       √       |      √       |          √          |
|      AzureDisk       |       √       |              |                     |
|        CephFS        |       √       |      √       |          √          |
|        Cinder        |       √       |              |                     |
|          FC          |       √       |      √       |                     |
|      FlexVolume      |       √       |      √       |                     |
|       Flocker        |       √       |              |                     |
|  GCEPersistentDisk   |       √       |      √       |                     |
|      Glusterfs       |       √       |      √       |          √          |
|       HostPath       |       √       |              |                     |
|        iSCSI         |       √       |      √       |                     |
| PhotonPersistentDisk |       √       |              |                     |
|       Quobyte        |       √       |      √       |          √          |
|         NFS          |       √       |      √       |          √          |
|         RBD          |       √       |      √       |                     |
|    VsphereVolume     |       √       |              | × (当pod并列时有效) |
|    PortworxVolume    |       √       |              |                     |
|       ScaleIO        |       √       |      √       |                     |
|      StorageOS       |       √       |              |                     |

#### 回收策略

- Retain (保留) ---- 手动回收
- Recycle (回收) ---- 基本擦除 ( rm -rf /thevolume/\* )
- Delete (删除) ---- 关联的存储资产 (例如 AWS EBS, GCE PD, Azure Disk 和 OpenStack Cinder卷) 将被删除

当前, 只有 NFS 和 HostPath 支持回收策略;

AWS EBS, GCE PD, Azure Disk 和 Cinder卷 支持删除策略;

#### 状态

卷可以处于以下的某种状态

- available (可用) ---- 一块限制资源, 还没有被任何声明绑定
- Bound (已绑定) ---- 卷已经被声明绑定
- Release (已释放) ---- 声明被删除, 但是资源还未被集群重新声明
- Failed (失败) --- 该卷的自动回收失败

命令行会显示绑定到 PV 的 PVC 名称

#### 持久化演示-NFS

> 安装nfs服务器

```
yum install -y nfs-utils rpcbind

mkdir /nfsdata

cat << EOF >> /etc/exports
/nfsdata *(rw,no_root_squash,no_all_squash,sync)

systemctl start rpcbind
systemctl start nfs
```

> 部署PV

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfspv1
spec:
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClasName: nfs
  nfs:
    path: /data/nfs
    server: 192.168.150.220
```

> 创建服务并使用PVC

```
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx
  serviceName: "nginx"
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16.0
        ports:
        - name: web
          containerPort: 80
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "nfs"
      resource:
        requests:
          storage: 1Gi
```

### 9.3.5 关于StatefulSet

- 匹配 Pod name (网络标识) 的模式为: ${statefulset名称}-${序号}

  比如示例中: web-0, web-1, web-2

- StatefulSet 为每个 Pod 副本创建了一个 DNS 域名, 这个域名格式为: ${podname}.{headless server name}, 也就意味着服务间是通过 Pod 域名来通信而非 Pod IP, 因为当 Pod 所在 Node 发生故障时, Pod 会被漂移到其它 Node 上, Pod IP 会发生变化, 但是 Pod 域名不会有变化

- StatefulSet 使用 Headless 服务控制着 Pod 的域名, 这个域名的 FQDN 为: ${servicename}.${namespace}.svc.cluster.local, 其中 "cluster.local"指的是集群的域名

- 根据 volumeClaimTemplates, 为每个 Pod 创建一个 pvc, pvc的命名规则匹配模式: ${volumeClaimTempate,name}-${pod_name}

  比如示例中的 volumeMounts.name=www, Podname=web-[0-2], 因此创建出来的 PVC 是 www-web-0, www-web-1, www-web-2

- 删除 Pod 不会删除其 PVC

  手动删除 PVC 将自动释放 PV

**StatefulSet的启动顺序**

- 有序部署: 部署 StatefulSet 时, 如果有多个 Pod 副本, 它们会被顺序的创建 (从0到N-1) 并且, 在下一个 Pod 运行之前 所有之前的 Pod  必须都是 Running 和 Ready 状态
- 有序删除: 当 Pod 被删除时, Pod 被终止的顺序是从N-1到0
- 有序扩展: 当对 Pod 执行扩展操作时, 与部署一样, 新加的 Pod 必须是之前的 Pod 都处于 Running 和 Ready 状态

**StatefulSet使用场景**

- 稳定的持久化存储, 即 Pod 重新调度后还是能访问到相同的持久化数据, 基于 PVC 来实现
- 稳定的网络标识, 即 Pod 重新调度后, PodName 和 HostName 不变
- 有序部署, 有序扩展, 基于 init containers 来实现
- 有序收缩



# 10 调度器

## 10.1 调度器说明

Scheduler 是 kubernetes 的调度器, 主要任务是把定义的 Pod 分配到集群的节点上;

Scheduler 需要考虑以下问题:

- 公平: 如何保证每个节点都能被分配到资源
- 资源高效利用: 集群所有资源最大化被使用
- 效率: 调度的性能要好, 能够尽快地对大批量的 Pod 完成调度工作
- 灵活: 允许用户根据自己的需求控制调度的逻辑

Scheduler 是作为单独的程序运行的, 启动之后会一直监听 APIServer, 获取 PodSpec.NodeName 为空的 Pod, 对每个 Pod 都会创建一个 binding, 表明该 Pod 应该放到那个节点上

### 10.1.1 调度过程

调度过程分为以下部分:

1. 首先是过滤掉不满足条件的节点, 这个过程称为 predicate
2. 然后对通过的节点按照优先级排序, 这个过程是 priority
3. 最后从中选择优先级最高的节点

如果中间任何一个步骤有错误, 就直接返回错误



**Predicate的算法**

- PodFitsResource: 节点上剩余的资源是否大于 Pod 请求的资源
- PotFitsHost: 如果 Pod 指定了 NodeName, 检查节点名称是否和 NodeName匹配
- PodFitsHostPorts: 节点上已经使用的 port 是否和 Pod 申请的 port 冲突
- PodSelectorNatches: 过滤掉和 pod 指定的 label 不匹配的节点
- NoDiskConflict: 已经 mount 的 volume 和 pod 指定的 volume 不冲突, 除非它们都是只读

如果在 predicate 过程中没有合适的节点, Pod 会一直在 `pending` 状态, 不断重试调度, 直到所有节点满足条件

经过这个步骤, 如果有多个节点满足条件, 就继续 priority 过程: 按照优先级大小对节点排序



优先级由一系列键值对组成, 键是该优先级项的名称, 值是它的权重(该项的重要性);

优先级包括:

- LeastRequestePriority: 通过计算 CPU 和 Memory 的使用率来决定权重, 使用率越低, 权重越高

  也就是: 优先级指标倾向于资源使用比例更低的节点

- BalancedResourceAllocation: 节点上 CPU 和 Memory 使用率越接近, 权重越高

  这个应该和 LeastRequestePriority 一起使用, 不能单独使用

- ImageLocalityPriority: 倾向于已经有要使用镜像的节点, 镜像总大小值越大, 权重越高

通过算法对所有优先级项目和权重进行计算, 得出最终的结果

### 10.1.2 自定义调度器

除了 Kubernetes 自带的调度器, 也可以自定义调度器;

通过 spec.schedulername 参数指定调度器的名字, 可以为 pod 选择某个调度器进行调度;

> 使用my-scheduler 替换默认的 default-scheduler

```
apiVersion: v1
kind: Pod
metadata:
  name: annotation-second-scheduler
  labels:
    name: multischeduler-example
spec:
  schedulername: my-scheduler
  containers:
  - name: pod-with-second-anotation-container
    image: nignx:1.16.0
```



## 10.2 调度亲和性

pod.spec.affinity.nodeAffinity.

- 硬亲和性: 硬亲和性不满足时, Pod 会置于 Pending 状态

  requiredDuringSchedulingIgnoreDuringExecution

- 软亲和性: 软亲和性不满足时, 会选择一个不匹配的节点

  preferredDuringSchedulingIgnoreDuringExecution

#### 硬亲和性

```yaml
apiVersion: v1
kind: Pod
metadata:
  name affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
  - name: with-node-affinity
    image: nginx:1.16.0
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoreDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:	# 集合选择器
          - key: kubernetes.io/hostname	# 节点Name
            operator: NotIn
            values:
            - k8s-node02
```

#### 软亲和性

```
apiVersion: v1
kind: Pod
metadata:
  name: affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
  - name: with-node-affinity
    image: nginx:1.16.0
  affinity:
    nodeAffinity:
      perferredDuringSchedulingIgnoreDuringExecuting:
      - weight: 1
        perference:
          matchExpressions:
          - key: source
            operator: In
            values:
            - qikqiak
```

#### 硬+软

```
apiVersion: v1
kind: Pod
metadata:
  name: affinity
  labels:
    app: node-affinity-pod
spec:
  containers:
  - name: with-node-affinity
    image: nginx:1.16.0
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoreDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kuernetes.io/hostname
            operator: NotIn
            values:
            - k8s-node02
      perferredDuringSchedulingIgnoreDuringExecuting:
      - weight: 1
        preference:
          matchExpressions:
          - key: source
            operator: In
            values:
            - qikiak
```

#### Operator 键值运算关系

- In: label 的值在某个列表中
- NotIn: label 的值不在某个列表中
- Gt: label 的值大于某个值
- Lt: label 的值小于某个值
- Exists: 某个 label 存在
- DoesNotExist: 某个 label 不存在

#### 调度策略比较

| 调度策略        | 匹配标签 | 操作符                                   | 拓扑域支持 | 调度目标                   |
| --------------- | -------- | ---------------------------------------- | ---------- | -------------------------- |
| nodeAffinity    | 主机     | In, NotIn, Exists, DoesNotExists, Gt, Lt | 否         | 指定主机                   |
| podAffinity     | Pod      | In, NotIn, Exists, DoesNotExists         | 是         | Pod与指定Pod在同一拓扑域   |
| podAnitAffinity | Pod      | In, NotIn, Exists, DoesNotExists         | 是         | Pod与指定Pod不在同一拓扑域 |



## 10.3 污点

### 10.3.1 Taint 和 Toleration

节点亲和性, 是 Pod 的一种属性 (偏好或硬性要求), 它使 Pod 被吸引到一类特定的节点; Taint 则相反, 它使节点能够排斥一类特定的 Pod;

Taint 和 Toleration 项目配合, 可以用来皮面 Pod 被分配到不合适的节点上; 每个节点都可以应用一个或多个 Taint, 这表示对于那些不能容忍这些 Taint 的 Pod, 是不会被该节点接受的; 如果 Toleration 应用于 Pod 上, 则表示这些 Pod 可以 (但不要求) 被调度到具有匹配 Taint 的节点上;

### 10.3.2 污点(Taint)

> 污点 (Taint) 的组成

使用 kubectl taint 命令可以给某个 Node 节点设置污点, Node 被设置上污点后就和 Pod 之间存在了一种相斥的关系, 可以让 Node 拒绝 Pod 的调度执行, 甚至将 Node 已经存在的 Pod 驱逐出去;

每个污点组成如下:

```
key=value:effect
```

每个污点有一个 key 和 value 作为污点的标签, 其中 value 可以为空, effect 描述污点的作用; 当前 taint effect 支持以下三个选项:

- NoSchedule: 表示K8S将不会讲 Pod 调度到具有该污点的 Node 上
- PreferNoSchedule: 表示 K8S 将尽量鼻片将 Pod 调度到具有该污点的 Node 上
- NoExecute: 表示 K8S 将不会将 Pod 调度到具有该污点的 Node 上, 同时会将 Node 上已经存在的 Pod 驱逐出去

> 污点的设置、查看和去除

```Shell
# 设置污点
kubectl taint nodes node1 key1=value1:NoSchedule

# 节点说明中, 查找 Taint 字段
kubectl describe pod pod-name

# 去除污点
kubectl taint nodes node1 key1:NoSchedule-
```

### 10.3.3 容忍(Tolerations)

设置了污点的 Node 将根据 taint 的 effect: NoSchedule、PreferNoScheduler、NoExecute 和 Pod 之间产生互斥的关系, Pod 将在一定程度上不会被调度到 Node 上; 

但可以在 Pod 上设置容忍 (Toleration), 意思是设置了容忍的 Pod 将可以容忍污点的存在, 可以被调度到存在污点的 Node 上;

**pod.spec.tolerations**

```yaml
# key, value, effect 要与 Node 上设置的 taint 保持一致
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoSchedule"
  tolerationSeconds: 3600	# 用于描述当 Pod 需要被驱逐时可以在 Pod 上继续保留运行的时间
- key: "key1"
  operation: "Equal"
  value: "value1"
  effect: "NoExecute"
- key: "key2"
  operator: "Exists"	# 值为 Exists 将会忽略 value 值
  effect: "NoSchedule"
```

> 当不指定 key 值时, 表示容忍所有的污点 key

```
tolerations:
- operator: "Exists"
```

> 当不指定 effect 值时, 表示容忍所有的污点作用

```
tolerations:
- key: "key"
  operator: "Exists"
```

> 有多个 Master 存在时, 防止资源浪费, 可以如下设置

```
kubectl taint nodes Node-Name node-role.kubernetes.io/master=:PerferNoSchedule
```



## 10.4 固定节点

#### 指定调度节点

> Pod.spec.nodeName 

将 Pod 直接调度到指定的 Node 节点上, 会跳过 Scheduler 的调度策略, 该匹配规则是强制匹配

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: myweb
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: myweb
    spec:
      nodeName: k8s-node01
      containers:
      - name: web
        image: nginx:1.16.0
        ports:
        - containersPort: 80
```

> Pod.spec.nodeSelector

通过 kubernetes 的 label-selector 机制选择节点, 由调度器调度策略匹配 label, 而后调度 Pod 到目标节点, 该匹配顾泽属于强制约束

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: myweb
spec:
  replicas: 2
  template:
    metadata
      labels:
        app: web
    spec:
      nodeSelector:
        type: backEndNode1
      containers:
      - name: myweb
        image: tomcat
        ports:
        - containerPort: 8080
```

