[toc]

# 11. 安全

## 11.1 机制说明

Kubernetes 作为一个分布式集群的管理工具, 保证集群的安全性是其一个重要的任务;

API Server 是集群内部各个组件通信的中介, 也是外部控制的入口;

所以 Kubernetes 的安全机制基本是围绕保护 API Server 来设计的;

Kubernetes 使用了认证 (Authentication)、鉴权 (Authorization)、准入控制 (AdmissionControl) 三步来保证 API Server 的安全;

## 11.2 认证

- HTTP Token 认证: 通过一个 Token 来识别合法用户

    - HTTP Token 的认证是用一个很长的特殊编码方式的并且难以被模仿的字符串 - Token 来表达客户的一种方式;

        Token 是一个字符串, 每一个 Token 对应一个用户名存储在 APIServer 能访问的文件中;

        当客户端发起 API 调用请求时, 需要在 HTTP Header 里放入 Token;

- HTTP Base 认证: 通过 用户名+密码 的方式认证

    - 用户名+: +密码 用 BASE64 算法进行编码后的字符串放在 HTTP Request 中的 Heather Authorization 域里发送给服务端, 服务端收到后进行解码, 获取用户名和密码

- HTTPS 证书认证: 基于 CA 根证书签名的客户端身份认证方式

> HTTPS证书认证过程



> 需要认证的节点

**两种类型**

- Kubernetes 组件对 APIServer 的访问: kubectl, Controller Manager, Scheduler, kubelet, kube-proxy
- Kubernetes 管理的 Pod 对容器的访问: Pod

**安全性说明**

- Controller Manager, Scheduler 与 APIServer在同一台机器, 所以直接使用 APIServer 的非安全端口访问, `--insecure-bind-address=127.0.0.1`
- kubectl, kubelet, kube-proxy 访问 APIServer 就都需要证书进行 HTTPS 双向认证

**证书颁发**

- 手动签发: 通过 k8s 集群的根ca进行签发HTTPS证书
- 自动签发: kubelet 首次访问 APIServer 时, 使用 token 做认证, 通过后, Controller Manager 会为 kubelet 生成一个证书, 以后的访问都是用证书做认证了

> kuberconfig

Kubernetes 文件包含集群参数 (CA证书, API Server地址), 客户端参数 (上面生成的证书和私钥), 集群 context 信息 (集群名称, 用户名);

Kubernetes 组件通过启动时指定不同的 kubeconfig 文件可以切换到不同集群;

> ServiceAccount

Pod 中的容器访问 APIServer;

因为 Pod 的创建、销毁是动态的, 所以要为它手动生成证书就不可行了;

Kubernetes 使用了 Service Account 解决 Pod 访问 APIServer 的认证问题;

> Secret 与 SA 的关系

Kubernetes 设计了一种资源对象叫做 Secret, 分为两类, 一种是用于 ServiceSccount 的 service-account-token, 另一种是用于保存用户自定义保密信息的 Opaque;

ServiceAccount 中用到包含的三个部分: Token、ca.crt、namespace

- token 是使用 APIServer 私钥签名的 JWT; 用于访问 APIServer 时, Server端认证
- ca.crt 根证书; 用于 Client 端验证 APIServer 发送的证书
- namespce, 标识这个 service-account-token 的作用域名空间

```shell
kubectl get secret --all-namespace
kubectl describe secret default-token-5gm9r --namespace=kube-system
```

默认情况下, 每个 namespace 都会有一个 ServiceAccount, 如果 Pod 在创建时没有 ServiceAccount, 就会使用 Pod 所属的 namespace 的 ServiceAccount;



## 11.3 鉴权

上面认证过程, 只是确认通信的双方都确认了对方是可信的, 可以互相通信;

而鉴权是确定请求方有哪些资源的权限;

APIServer 目前支持以下几种授权策略 (通过 APIServer 的启动参数 "--authorization-mode" 设置)

- AlwaysDeny: 表示拒绝所有的请求, 一般用于测试
- AlwaysAllow: 允许接收所有的请求, 如果集群不需要授权流程, 则可以采用该策略
- ABAC (Attribute-Based Access Control): 基于属性的访问控制, 表示使用用户配置的授权规则对用户请求进行匹配和控制
- Webhook: 通过调用外部 REST 服务队用户进行授权
- RBAC (Role-Based Access Control): 基于角色的访问控制, 现行默认规则

#### RBAC授权模式

RBAC (Role-Based Access Control) 基于角色的访问控制, 在 Kubernetes 1.5 中引入, 现行版本成为默认标准;

相对其他访问控制方式, 拥有以下优势:

- 对集群中的资源和非资源均拥有完整的覆盖
- 整个 RBAC 完全由几个 API 对象完成, 同其它 API 对象一样, 可以用 kubectl 或 API 进行操作
- 可以在运行时进行调整, 无需重启 APIServer

> RBAC的API资源对象说明

RBAC 引入了 4 个新的顶级资源对象: Role、ClusterRole、RoleBinding、ClusterRoleBinding, 4种对象类型均可以通过 kubectl 与 API 操作;

APIServer 会把客户端证书的 CN 字段作为 User, 把 names.0 字段作为 Group;

kubelet 使用 TLS Bootstaping 认证时, APIServer 可以使用 Bootstrap Tokens 或者 Token authentication file 验证 =token, 无论哪一种, Kubernetes 都会为 token 绑定一个默认的 User 和 Group

Pod 使用 ServiceAccount 认证时, service-account-token 中 JWT 会保存 User 信息

有了用户信息, 再创建一对角色/角色绑定(集群角色/集群角色绑定)资源对象, 就可完成权限绑定了

> Role and ClusterRole

在 RBAC API 中, Role 表示一组规则权限, 权限只会增加(累计加权), 不存在一个资源一开始就有很多权限而通过 RBAC 对其进行减少的操作;

Role 可以定义在一个 namespace 中, 如果想要跨 namespace 则可以创建 ClusterRole

```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: pod-reader
  namespace: default
rules:
- apiGroup: [""] # "" indicates the core API group
  resource: ["pods"]
  verbs: ["get", "watch", "list"]
```

ClusterRole 具有与 Role 相同的权限角色控制能力, 不同的是 ClusterRole 是集群级别的, ClusterRole 可以用于:

- 集群级别的资源控制(例如 node 访问权限)
- 非资源型 endpoint(例如 /healthz 访问)
- 所有命名空间资源控制(例如 pods)

```yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
meadata:
  # "namespace" omitted since ClusterRoles are not namespaced
  name: secret-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```

#### RoleBinding and ClusterRoleBinding

RoleBinding 可以将角色中定义的权限授予用户或用户组, RoleBinding 包含一组权限列表(subjects), 权限列表中包含有不同形式的待授予权限资源类型(users, groups, or service accounts);

RoleBinding 同样包含对被 Bind 的 Role 引用;

RoleBinding 适用于某个明明空间内授权, 而 ClusterRoleBinding 适用于集群范围内的授权;

将 default 命名空间的 pod-reader Role 授予 jane 用户, 此后 jane 用户在 default 命名空间中将具有 pod-reader 的权限;

```yaml
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

RoleBding 同样可以引用 ClusterRole 来对当前 namespace 内用户、用户组或 ServiceAccount 进行授权, 这种操作运行集群管理员在整个集群内定义一些通用的 ClusterRole, 然后在不同的 namespace 中使用 RoleBinding 来引用;

例如, 以下 RoleBinding 引用了一个 ClusterRole, 这个 ClusterRole 具有整个集群内对 secrets 的访问权限;

但是其授权用户 dave 只能访问 development 空间中的 secrets(因为 RoleBinding 定义在 development 命名空间)

```
# This role inding allows "dave" to read secrets in the "development" namespace.
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: read-secrets
  namespace: developments # This onlu grants permissions within the "development" namespace.
subjects:
- kind: User
  name: dave
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

使用 ClusterRoleBinding 可以对整个集群中的所有命名空间资源权限进行授权; 

以下 ClusterRoleBinding 样例展示了授权 manager 组内所有用户在全部命名空间中对 secrets 进行访问;

```
# This cluster role binding allows anyone in the "manager" group to read secrets in any namespace.
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

#### Resource

Kubernetes 集群内一些资源一般以其名称字符串来表示, 这些字符串一般会在 API 的 URL 地址中出现;

同时某些资源也会包含子资源, 例如 logs 资源就属于 Pods 的子资源, API 中 URL 样例如下

```js
GET /api/v1/namespace/{namespace}/pods/{name}/log
```

如果要在 RBAC 授权模型中控制这些子资源的访问权限, 可以通过 / 分隔符来实现, 以下是一个定义 pods 资源 logs 访问权限的 Role 定义样例

```
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: pod-and-pod-logs-reader
  namespace: default
rules:
- apiGroups: [""]
  resource: ["pods/log"]
  verbs: ["get", "list"]
```



#### to Subjects

RoleBinding 和 ClusterRoleBinding 可以将 Role 绑定到 Subjects; Subjects 可以 groups、users 或者 service accounts;

Subjects 中 Users 使用字符串表示, 它可以是一个普通的名字的字符串, 如"alice"; 也可以是 emai 格式的邮箱地址; 甚至是一组字符串形式的数字ID;

但是 Users 的前缀 system: 是系统保留的, 集群管理员应该确保普通用户不会使用这个前缀格式;

Groups 书写格式与 Users 相同, 都为一个字符串, 并且没有特定的格式要求; 同样 system: 前缀为系统保留;



## 11.4 准入控制

准入控制是 APIServer的插件集合, 通过添加不同的插件, 实现额外的准入控制规则;

甚至于 APIServer 的一些主要的功能都需要通过 Adminssion Controllers 实现, 比如 ServiceAccount;

官方文档上有一份针对不同版本的的准入控制器推荐列表, 其中最新的 1.14 的推荐列表是:

```
NamespaceLifecycle, LimitRanger, ServiceAccount, DefaultStorageClass, DefaultTolerationSeconds, MutatingAdmissionWebhook, ValidatingAdminssionWebhook, ResourceQuota
```

列举几个插件的功能:

- namespaceLifecycle: 防止在不存在的 namespace 上创建对象, 防止删除系统预置 namespace, 删除 namespace 时, 连带删除它的所有资源对象
- LimitRanger: 确保请求的资源不会超过资源所在的 Namespace 的 LimitRange 的限制
- ServiceAccount: 实现了自动化添加 ServiceAccount
- ResourceQuota: 确保请求的资源不会超过资源的 ResourceQuota 限制



# 12. Helm

Helm 为团队提供了在 Kubernetes 内部创建、安装和管理应用程序时需要协作的工具，有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。

> Helm 解决了什么痛点

K8S上的应用对象，都是由特定的资源描述组成，包括deployment、service等。都保存各自文件中或者集中写到一个配置文件。然后kubectl apply –f 部署。如果应用只由一个或几个这样的服务组成，这种部署方式足够了。

而对于一个复杂的应用，会有很多类似上面的资源描述文件，例如微服务架构应用，组成应用的服务可能多达十个，几十个。如果有更新或回滚应用的需求，可能要修改和维护所涉及的大量资源文件，而这种组织和管理应用的方式就显得力不从心了。

有了 Helm，开发者可以：

- 查找要安装和使用的预打包软件（Chart）

- 轻松创建和托管自己的软件包
- 将软件包安装到任何 K8s 集群中
- 查询集群以查看已安装和正在运行的程序包
- 更新、删除、回滚或查看已安装软件包的历史记录

> Helm 组件及相关术语

**helm**

- Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。

**Chart**

- Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。

**Repoistory**

- Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。

**Release**

- 使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release。可以理解为 Helm 使用 Chart 包部署的一个应用实例。

## 12.1 Helm部署

```
wget https://get.helm.sh/helm-v3.6.0-linux-amd64.tar.gz

chmod 755 linux-amd64/helm
cp linux-amd64/helm /usr/local/bin
```

> 配置国内chart仓库

- 微软仓库: http://mirror.azure.cn/kubernetes/charts
- 阿里云仓库: https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

```
# 添加源
helm repo add stable http://mirror.azure.cn/kubernetes/charts
helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
helm repo update
```

```
# 查看源
helm repo list

# 查看所有源中chart(软件包)
helm search repo
# 查看所有源中包含stable的chart
helm search repo stable

# 删除源
helm repo remove aliyun
```

## 12.2 Helm基本使用

```
# 查找chart
helm search repo mysql

# 查看chart信息
helm show chart stable/mysql
# 查看chart全部信息
helm show all stable/mysql
# 查看chart的yaml
helm show value stable/mysql

# 安装chart
helm install my-db stable/mysql

# 查看软件状态
helm status my-db
```

```
[root@k8s-master01 ~]# helm list
NAME 	NAMESPACE	REVISION	UPDATED                                	STATUS  	CHART      	APP VERSION
my-db	default  	1       	2021-06-20 15:17:43.345266243 +0800 CST	deployed	mysql-1.6.9	5.7.30

[root@k8s-master01 mysql]# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
my-db-mysql-86b64bdf75-bpnl9   0/1     Pending   0          95m
```

> 下载一个chart

```
[root@k8s-master01 mysql]# helm pull stable/mysql --untar


[root@k8s-master01 mysql]# tree mysql
mysql
├── Chart.yaml
├── README.md
├── templates
│   ├── configurationFiles-configmap.yaml
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── initializationFiles-configmap.yaml
│   ├── NOTES.txt
│   ├── pvc.yaml
│   ├── secrets.yaml
│   ├── serviceaccount.yaml
│   ├── servicemonitor.yaml
│   ├── svc.yaml
│   └── tests
│       ├── test-configmap.yaml
│       └── test.yaml
└── values.yaml
```

#### Helm常用命令

| 命令       | 描述                                                         |
| ---------- | ------------------------------------------------------------ |
| create     | 创建一个chart并指定名称                                      |
| dependency | 管理chart依赖                                                |
| get        | 下载一个release; 可用子命令: all, hooks, manifest, notes, values |
| history    | 获取release历史                                              |
| install    | 安装一个chart                                                |
| list       | 列出release                                                  |
| package    | 将chart目录打包到chart存档文件中                             |
| pull       | 从远程仓库下载chart并解压到本地; helm pull stable/mysql --untar |
| repo       | 添加, 列出, 移除, 更新, 建索 chart仓库; 可用子命令: add, index, list, remove, update |
| rollback   | 回滚之前版本                                                 |
| search     | 根据关键字搜索chart; 可用子命令: hub, repo                   |
| show       | 查看chart详细信息; 可用子命令: all, chart, readme, value     |
| status     | 显示已命名版本的状态                                         |
| template   | 本地呈现模板                                                 |
| uninstall  | 卸载一个release                                              |
| upgrade    | 更新一个release                                              |
| version    | 查看helm客户端版本                                           |

## 12.3 Chart使用

> 创建一个chart项目

```
[root@k8s-master01 ~]# helm create mychart

[root@k8s-master01 ~]# tree mychart/
mychart/
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── hpa.yaml
│   ├── ingress.yaml
│   ├── NOTES.txt
│   ├── serviceaccount.yaml
│   ├── service.yaml
│   └── tests
│       └── test-connection.yaml
└── values.yaml
```

```shell
helm install my-release .
```





