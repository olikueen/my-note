[toc]

```shell
mkdir -pv /data/nfs/{redis,mysql,zookeeper,activemq,mongodb,cassandra}
```

### MySQL

- 能够部署一个 一主多从 的mysql集群
- 没有提供数据备份能力

**添加helm源**

```shell
helm repo add stable http://mirror.azure.cn/kubernetes/charts
helm repo add aliyun https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts
helm repo add aliyuncs https://apphub.aliyuncs.com
```

**拉取mysqlha安装包**

```shell
helm pull aliyuncs/mysqlha --untar
cd mysqlha
```

**配置 values.yaml** 

```shell
mysqlha:
  # 修改mysql集群的副本数, 默认是3(1master 2slave)
  replicaCount: 2
  # root用户密码
  mysqlRootPassword: 'mWj1#tYbSrB#oJ2K'
  # 编辑mysql配置文件内容
  configFiles:
    master.cnf: |
      [mysqld]
      log-bin
      skip_name_resolve
      character-set-server = utf8mb4
      default-time_zone = '+8:00'
      innodb_large_prefix=on
      innodb_file_format = BARRACUDA
      innodb_file_per_table=true
      collation-server = utf8mb4_unicode_ci  
      init_connect='SET NAMES utf8mb4'  
      skip-character-set-client-handshake = true  
      max_connections=2000
      symbolic-links=0
      sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'
      [client]
      default-character-set=utf8mb4
      [mysql]
      default-character-set=utf8mb4
    slave.cnf: |
      [mysqld]
      character-set-server = utf8mb4
      default-time_zone = '+8:00'
      innodb_large_prefix=on
      innodb_file_format = BARRACUDA
      innodb_file_per_table=true
      collation-server = utf8mb4_unicode_ci  
      init_connect='SET NAMES utf8mb4'  
      skip-character-set-client-handshake = true  
      max_connections=2000
      symbolic-links=0
      sql_mode='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'
      [client]
      default-character-set=utf8mb4
      [mysql]
      default-character-set=utf8mb4
# mysql数据持久化配置
persistence:
  enabled: false	# 生产配置成true
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, azure-disk on
  ##   Azure, standard on GKE, AWS & OpenStack)
  ##
  storageClass: "mysql-nfs"
  accessModes:
  - ReadWriteOnce
  size: 10Gi
  annotations: {}
```

**修改 templates/statefulset.yaml**

```shell
# StatefulSet的apiVersion已经更新为 apps/v1
apiVersion: apps/v1
kind: StatefulSet
# 需要增加一个selector否则会报错
spec:
  serviceName: {{ template "fullname" . }}
  replicas: {{ .Values.mysqlha.replicaCount }}
  selector:
    matchLabels:
      app: {{ template "fullname" . }}
# 如果要使用hostPath, 需要修改这一段
{{- else }}
      - name: data
        emptyDir: {}
{{- end }}
```

**安装mysqlha**

```shell
helm install mysql .
```

```shell
NAME: mysql
LAST DEPLOYED: Mon Aug 16 10:53:30 2021
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
The MySQL cluster is comprised of 2 MySQL pods: 1 master and 1 slaves. Each instance is accessible within the cluster through:

    <pod-name>.mysql-mysqlha

`mysql-mysqlha-0.mysql-mysqlha` is designated as the master and where all writes should be executed against. Read queries can be executed against the `mysql-mysqlha-readonly` service which distributes connections across all MySQL pods.

To connect to your database:

1. Obtain the root password: 

    kubectl get secret --namespace default mysql-mysqlha -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo

2. Run a pod to use as a client:

    kubectl run mysql-client --image=mysql:5.7.13 -it --rm --restart='Never' --namespace default -- /bin/sh

3. To connect to Master service (read/write):

    mysql -h mysql-mysqlha-0.mysql-mysqlha -u root -p

4. To connect to slave service (read-only):
   
   mysql -h mysql-mysqlha-readonly -u root -p
```

**获取mysql root密码**

```shell
kubectl get secret --namespace default mysql-mysqlha -o jsonpath="{.data.mysql-root-password}" | base64 --decode; echo
ecAwiULEehcM
```

mysql 备份

```yaml
```

### redis

**拉取redis-ha安装包**

```
helm pull stable/redis-ha --untar
cd redis-ha
```

**配置 value.yaml**

```shell
## replicas number for each component
# 节点个数(3 => 1 master, 2 slave, 3 sentinel)
replicas: 3

# 配置Haproxy, 代理访问请求
haproxy:
  enabled: true
  # Enable if you want a dedicated port in haproxy for redis-slaves
  # 开启读写分离, 把读请求转发到 6380端口
  readOnly:
    enabled: true
    port: 6380
  replicas: 2

## Configures redis with AUTH (requirepass & masterauth conf params)
# 配置reids的密码
auth: true
redisPassword: redis123

# 配置reids数据卷
persistentVolume:
  enabled: true
  ## redis-ha data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: "pv-redis"
  accessModes:
    - ReadWriteOnce
  size: 2Gi
  annotations: {}
  # reclaimPolicy per https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming
  reclaimPolicy: ""
```

**安装redis-ha**

```shell
kubectl create ns redis
helm install -n redis redis .
```

```shell
NAME: redis
LAST DEPLOYED: Mon Aug 16 15:33:31 2021
NAMESPACE: redis
STATUS: deployed
REVISION: 1
NOTES:
Redis can be accessed via port 6379 and Sentinel can be accessed via port 26379 on the following DNS name from within your cluster:
redis-redis-ha.redis.svc.cluster.local

To connect to your Redis server:
1. Run a Redis pod that you can use as a client:

   kubectl exec -it redis-redis-ha-server-0 sh -n redis

2. Connect using the Redis CLI:

  redis-cli -h redis-redis-ha.redis.svc.cluster.local
```

**连接方式**

```shell
# 读写
redis-cli -h redis-redis-ha-haproxy.redis.svc.cluster.local -p 6379
# 只读
redis-cli -h redis-redis-ha-haproxy.redis.svc.cluster.local -p 6380
# 哨兵
redis-cli -h redis-redis-ha.redis.svc.cluster.local -p 26379
```

### mongodb

**拉取mongodb-replicaset包**

```shell
helm pull aliyuncs/mongodb-replicaset --untar
```

**修改mongodb配置**

```shell
# 配置登录认证
auth:
  enabled: true
  existingKeySecret: ""
  existingAdminSecret: ""
  existingMetricsSecret: ""
  adminUser: admin
  adminPassword: admin@123
# 配置存储卷, 需要先创建pv-mongodbc
persistentVolume:
  enabled: true
  ## mongodb-replicaset data Persistent Volume Storage Class
  ## If defined, storageClassName: <storageClass>
  ## If set to "-", storageClassName: "", which disables dynamic provisioning
  ## If undefined (the default) or set to null, no storageClassName spec is
  ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
  ##   GKE, AWS & OpenStack)
  ##
  storageClass: "pv-mongodb"
  accessModes:
    - ReadWriteOnce
  size: 2Gi
  annotations: {}

```



### cassandra

```shell
# 绑定存储卷, 需要先创建 pv-cassandra 存储类
persistence:
  enabled: true
  storageClass: "pv-cassandra"
  annotations:
  accessModes:
    - ReadWriteOnce
  size: 2Gi
# 配置数据库密码
dbUser:
  user: cassandra
  forcePassword: true
  password: admin@123
# 开启 exporter metrics 接口
metrics:
  enabled: true
  image:
    registry: docker.io
    pullPolicy: IfNotPresent
    repository: bitnami/cassandra-exporter
    tag: 2.3.2-debian-10-r3

```

```shell
helm install cassandra . -f ./values-production.yaml -n middlerware
```

```shell
NAME: cassandra
LAST DEPLOYED: Thu Aug 19 09:47:07 2021
NAMESPACE: middlerware
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

Cassandra can be accessed through the following URLs from within the cluster:

  - CQL: cassandra.middlerware.svc.cluster.local:9042
  - Thrift: cassandra.middlerware.svc.cluster.local:9160

To get your password run:

   export CASSANDRA_PASSWORD=$(kubectl get secret --namespace middlerware cassandra -o jsonpath="{.data.cassandra-password}" | base64 --decode)

Check the cluster status by running:

   kubectl exec -it --namespace middlerware $(kubectl get pods --namespace middlerware -l app=cassandra,release=cassandra -o jsonpath='{.items[0].metadata.name}') nodetool status

To connect to your Cassandra cluster using CQL:

1. Run a Cassandra pod that you can use as a client:

   kubectl run --namespace middlerware cassandra-client --rm --tty -i --restart='Never' \
   --env CASSANDRA_PASSWORD=$CASSANDRA_PASSWORD \
   --labels="cassandra-client=true" \
   --image docker.io/bitnami/cassandra:3.11.6-debian-10-r0 -- bash

2. Connect using the cqlsh client:

   cqlsh -u cassandra -p $CASSANDRA_PASSWORD cassandra


Note: Since NetworkPolicy is enabled, only pods with label
"cassandra-client=true"
will be able to connect to Cassandra.
```

### minio

此方案适用 用户不提供专业存储的情况, 使用本地hostPath作为存储;

**创建minio数据目录**

```shell
mkdir -pv /data/minio
```

**给需要运行minio的节点打标签**

```shell
kubectl label nodes k8s-master01 minio-server=true
kubectl label nodes k8s-master02 minio-server=true
kubectl label nodes k8s-node01 minio-server=true
kubectl label nodes k8s-node02 minio-server=true
```

**安装minio**

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: minio
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: minio
  namespace: minio
  labels:
    app: minio
spec:
  selector:
    matchLabels:
      app: minio
  template:
    metadata:
      labels:
        app: minio
    spec:
      nodeSelector:
        # 配置节点选择器, 只有在 minio-server=true 标签的节点上, 部署minio pod
        minio-server: "true"
      hostNetwork: true
      volumes:
      - name: storage
        hostPath:
          # 配置物理机minio存储路径
          path: /data/minio
      containers:
      - name: minio
        # minio console 的登录用户(MINIO_ACCESS_KEY)和密码(MINIO_SECRET_KEY)
        env:
        - name: MINIO_ACCESS_KEY
          value: "v9rwqYzXXim6KJKeyPm344"
        - name: MINIO_SECRET_KEY
          value: "0aIRBu9KU7gAN0luoX8uBE1eKWNPDgMnkVqbPC"
        image: minio/minio:RELEASE.2020-06-14T18-32-17Z
        # Unfortunately you must manually define each server. Perhaps autodiscovery via DNS can be implemented in the future.
        args:
        - server
        # 配置各个minio节点
        - http://k8s-master01/data/minio
        - http://k8s-master02/data/minio
        - http://k8s-node01/data/minio
        - http://k8s-node02/data/minio
        ports:
        - containerPort: 9000
        volumeMounts:
        - name: storage
          mountPath: /data/minio/
---
apiVersion: v1
kind: Service
metadata:
  name: minio
  namespace: minio
spec:
  type: NodePort
  ports:
  - name: web
    nodePort: 30900
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app: minio
```

#### mc客户端使用方法

**下载mc客户端**

```shell
wget http://dl.minio.org.cn/client/mc/release/linux-amd64/mc
chmod 777 mc
mv mc /usr/local/bin
```

**添加 mc 配置文件, 两串随机字符串分别是 MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY**

```shell
# mc config host add minio节点别名 minio节点url MINIO_ACCESS_KEY MINIO_SECRET_KEY --api s3v4
mc config host add minio http://192.168.101.141:9000 v9rwqYzXXim6KJKeyPm344 0aIRBu9KU7gAN0luoX8uBE1eKWNPDgMnkVqbPC --api s3v4
# 配置文件存储在 ~/.mc 中, 查看配置
mc config host list
```

**创建存储桶**

```shell
# mc mb minio节点别名/存储桶名
[root@k8s-master01 aa]# mc mb minio/test2
Bucket created successfully `minio/test2`.

# mc mb minio节点别名/存储桶名/文件路径
[root@k8s-master01 aa]# mc mb minio/test3/a/b/c
Bucket created successfully `minio/test3/a/b/c`.
```

查看minio中存储桶

```shell
# mc ls minio节点别名
[root@k8s-master01 aa]# mc ls minio
[2021-08-20 13:52:02 CST]     0B pvc-f84801b2-c9e2-4a06-a257-e40eb0e8e0de/
[2021-08-19 15:51:36 CST]     0B test/
[2021-08-20 17:00:31 CST]     0B test2/
[2021-08-20 17:02:11 CST]     0B test3/

[root@k8s-master01 aa]# mc ls minio/test3
[2021-08-20 17:02:59 CST]     0B a/
```

上传文件

```shell
[root@k8s-master01 aa]# date > testfile.txt

[root@k8s-master01 aa]# mc cp testfile.txt minio/test3/a/
testfile.txt:                     43 B / 43 B ┃▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓┃ 1.58 KiB/s 0s 

[root@k8s-master01 aa]# mc ls minio/test3/a
[2021-08-20 17:05:18 CST]    43B testfile.txt
[2021-08-20 17:06:51 CST]     0B b/
```

删除文件

```shell
[root@k8s-master01 aa]# mc rm minio/test3/a/testfile.txt
Removing `minio/test3/a/testfile.txt`.
```

#### 权限设置

要想通过url直接下载minio中的文件, 需要给minio的桶配置权限

权限默认有 none, download, upload, public 种, 配置相应的权限, 可以开启对应的 免认证访问;

```shell
# 我预先在test桶中上传了一个文件
[root@k8s-master01 ~]# mc cp kuboard-v3.yaml minio/test

# 查看权限
[root@k8s-master01 tmp]# mc policy get minio/test
Access permission for `minio/test` is `none`
# 这种时候, 无论上传还是下载, 都需要先通过权限验证才行

# 设置权限, 给minio的test桶开启download权限
[root@k8s-master01 ~]# mc policy set download minio/test
Access permission for `minio/test` is set to `download`

# 这时候就可以免密下载文件了 http://192.168.101.141:9000/minio/download/<存储桶>/<文件路径>?token=
[root@k8s-master01 tmp]# wget http://192.168.101.141:9000/minio/download/test/kuboard-v3.yaml?token=
```



### zookeeper

```shell
# 副本数, 生产环境需要3节点zookeeper
replicaCount: 3

# 存储卷配置
persistence:
  enabled: true
  storageClass: "pv-zookeeper"
  accessModes:
    - ReadWriteOnce
  size: 2Gi
  annotations: {}

# zookeeper expoter metrics接口配置
metrics:
  enabled: true
  image:
    registry: docker.io
    repository: bitnami/zookeeper-exporter
    tag: 0.1.3-debian-10-r17
    pullPolicy: IfNotPresent

# kube-prometheus的ServiceMonitor配置
  serviceMonitor:
    enabled: false
    namespace: monitoring
    interval: 10s
    scrapeTimeout: 10s
    selector:
      prometheus: my-prometheus
```

```shell
helm install zookeeper . -n middlerware
```

```shell
NAME: zookeeper
LAST DEPLOYED: Wed Aug 18 15:59:46 2021
NAMESPACE: middlerware
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
** Please be patient while the chart is being deployed **

ZooKeeper can be accessed via port 2181 on the following DNS name from within your cluster:

    zookeeper.middlerware.svc.cluster.local

To connect to your ZooKeeper server run the following commands:

    export POD_NAME=$(kubectl get pods --namespace middlerware -l "app.kubernetes.io/name=zookeeper,app.kubernetes.io/instance=zookeeper,app.kubernetes.io/component=zookeeper" -o jsonpath="{.items[0].metadata.name}")
    kubectl exec -it $POD_NAME -- zkCli.sh

To connect to your ZooKeeper server from outside the cluster execute the following commands:

    kubectl port-forward --namespace middlerware svc/zookeeper 2181:2181 &
    zkCli.sh 127.0.0.1:2181
```


