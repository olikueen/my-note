redis集群(Redis Cluster), 是一个分布式、容错的Redis实现, 它由多个Redis节点组成, 在多个Redis节点之间进行数据共享; 集群可以使用的功能是普通单机Redis所能使用的功能的一个子集, 它提供了复制和故障转移功能;

Redis集群中不存在中心节点或代理节点, 而且不支持那些需要同时处理多个键的Redis命令, 因为要执行这些命令, 需要在多个Redis节点之间移动数据, 并且在高负载的情况下, 执行这些命令会降低Redis集群的性能, 并出现不可预料的问题;

**好处**

- 可以实现对Redis的水平扩容, 即启动N个redis节点, 将整个数据库分部存储在这N个节点中, 每个节点存储总数据的1/N;
- 当集群中有部分节点失效或者无法提供服务的时候, 它仍然可以继续完成相关的命令请求;
- Redis     集群的使用可以解决高并发、大数据量的问题;

**缺点**

- 不再支持多键操作

  ```
  192.168.30.137:6379> mset A a H h O o Z z
  (error) CROSSSLOT Keys in request don't hash to the same slot
  ```

- 多键的redis事务不再支持, 一旦操作的key超出单个slot, 事务就会被打断

  ```
  192.168.30.140:6379> MULTI
  OK
  192.168.30.140:6379> set Z z
  QUEUED
  192.168.30.140:6379> set H h
  -> Redirected to slot [2508] located at 192.168.30.137:6379
  OK
  192.168.30.137:6379> EXEC
  (error) ERR EXEC without MULTI
  ```

### 概念

**节点**

- 一个Redis集群通常由多个节点(Node)组成, 在没有搭建Redis集群之前, 每个节点都是相互独立的, 彼此之间没有任何联系, 每个节点都只包含在自己的集群中, 只有将多个独立的节点连接在一起, 才能组建一个可以工作的集群;

**槽(slot)**

- Redis集群为了能够存储大量的数据信息, 采用分片的方式将大量数据保存在数据库中, 这个数据库被划分为16384个槽(Slot);

- 可以把槽理解为一个数字, 槽是有一定范围的, 在Redis中的范围是0-16383;

- 每个槽映射一个大数据子集, 

 

### 配置Redis集群

**安装ruby环境**

集群构建工具是ruby脚本

```
dnf install -y ruby rubygems
```

**配置redis**

```
# 开启集群模式
cluster-enabled yes

# 指定集群配置文件, 这个文件不需要手动编辑, 它由redis节点自动创建和更新
cluster-config-file nodes-6379.conf

# 设置集群节点超时时间(毫秒), 超时自动切换主从
cluster-node-timeout 15000

# 当集群中有整段slot故障时, 集群不对外提供服务, 这样是不合适的, 需要设置为no;
cluster-require-full-coverage no
```

**启动redis服务 redis01 redis02 redis03**

```
redis-server /etc/redis/redis-6379.conf
redis-server /etc/redis/redis-6380.conf

ps -ef | grep redis
root        1387       1  0 23:12 ?        00:00:00 redis-server 0.0.0.0:6379 [cluster]
root        1393       1  0 23:12 ?        00:00:00 redis-server 0.0.0.0:6380 [cluster]
root        1399    1222  0 23:12 pts/0    00:00:00 grep --color=auto redis
```

**构建集群 redis01**

```
cd redis-6.0.8/src/

# redis 6.x
redis-cli --cluster create --cluster-replicas 1 \
192.168.30.137:6379 \
192.168.30.137:6380 \
192.168.30.139:6379 \
192.168.30.139:6380 \
192.168.30.140:6379 \
192.168.30.140:6380

# redis 5.x
./redis-trib.rb create --replicas 1 IP:Port
```

```
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 192.168.30.139:6380 to 192.168.30.137:6379
Adding replica 192.168.30.140:6380 to 192.168.30.139:6379
Adding replica 192.168.30.137:6380 to 192.168.30.140:6379
...
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
.
>>> Performing Cluster Check (using node 192.168.30.137:6379)
...
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
```

**使用client连接集群**

```
redis-cli -c -p 6379

127.0.0.1:6379> set k1 v1
-> Redirected to slot [12706] located at 192.168.30.140:6379
OK
# k1 被存储到192.168.30.140:6379的12706插槽上

192.168.30.140:6379> get k1
"v1"

# 查看集群节点信息
192.168.30.140:6379> cluster nodes
68a8c44fbab09bc401507cf8365ed87248918783 192.168.30.137:6379@16379 master - 0 1604244778000 1 connected 0-5460
da30388b4802e0aa64ef340b6641eaf40e20c20a 192.168.30.140:6379@16379 myself,master - 0 1604244777000 5 connected 10923-16383
7953d98051c690031521dbd888a4455bc7363e08 192.168.30.139:6380@16380 slave 68a8c44fbab09bc401507cf8365ed87248918783 0 1604244776000 1 connected
8b96e6e5e7be7a31fd3ccbf7055af07990f2ef90 192.168.30.139:6379@16379 master - 0 1604244777000 3 connected 5461-10922
dd44b3f386865561c30219aae9466cb2959a9df5 192.168.30.137:6380@16380 slave da30388b4802e0aa64ef340b6641eaf40e20c20a 0 1604244778996 5 connected
797fdce256f9d0c90342716087cc028a2a0c6a14 192.168.30.140:6380@16380 slave 8b96e6e5e7be7a31fd3ccbf7055af07990f2ef90 0 1604244776983 3 connected
```

redis集群如何分配节点:

- 一个集群至少有三个主节点
- --cluster-replicas 1 表示希望为集群中每个主节点创建一个从节点
- 分配原则: 见谅保证每个主节点运行在不同的IP地址, 每个从节点和主节点不在同一个IP上

# 故障恢复

- 主节点下线后, 从节点自动升为主节点

  ```
  # 关闭192.168.30.139:6379主节点
  127.0.0.1:6379> CLUSTER NODES
  7953d98051c690031521dbd888a4455bc7363e08 192.168.30.139:6380@16380 slave 68a8c44fbab09bc401507cf8365ed87248918783 0 1604245482000 1 connected
  dd44b3f386865561c30219aae9466cb2959a9df5 192.168.30.137:6380@16380 slave da30388b4802e0aa64ef340b6641eaf40e20c20a 0 1604245483191 5 connected
  797fdce256f9d0c90342716087cc028a2a0c6a14 192.168.30.140:6380@16380 master - 0 1604245481177 7 connected 5461-10922
  8b96e6e5e7be7a31fd3ccbf7055af07990f2ef90 192.168.30.139:6379@16379 master,fail - 1604245435816 1604245430000 3 disconnected
  da30388b4802e0aa64ef340b6641eaf40e20c20a 192.168.30.140:6379@16379 master - 0 1604245482183 5 connected 10923-16383
  68a8c44fbab09bc401507cf8365ed87248918783 192.168.30.137:6379@16379 myself,master - 0 1604245481000 1 connected 0-5460
  ```

- 故障节点恢复后, 转为从节点身份

  ```
  # 重新启动192.168.30.139:6379节点
  127.0.0.1:6379> CLUSTER NODES
  7953d98051c690031521dbd888a4455bc7363e08 192.168.30.139:6380@16380 slave 68a8c44fbab09bc401507cf8365ed87248918783 0 1604245587000 1 connected
  dd44b3f386865561c30219aae9466cb2959a9df5 192.168.30.137:6380@16380 slave da30388b4802e0aa64ef340b6641eaf40e20c20a 0 1604245586998 5 connected
  797fdce256f9d0c90342716087cc028a2a0c6a14 192.168.30.140:6380@16380 master - 0 1604245588006 7 connected 5461-10922
  8b96e6e5e7be7a31fd3ccbf7055af07990f2ef90 192.168.30.139:6379@16379 slave 797fdce256f9d0c90342716087cc028a2a0c6a14 0 1604245589014 7 connected
  da30388b4802e0aa64ef340b6641eaf40e20c20a 192.168.30.140:6379@16379 master - 0 1604245588000 5 connected 10923-16383
  68a8c44fbab09bc401507cf8365ed87248918783 192.168.30.137:6379@16379 myself,master - 0 1604245587000 1 connected 0-5460
  ```

- 如果某段slot主从节点都下线, 这段slot将不再可用

  ```
  # 下线192.168.30.139:6379 192.168.30.140:6380节点
  127.0.0.1:6379> CLUSTER NODES
  7953d98051c690031521dbd888a4455bc7363e08 192.168.30.139:6380@16380 slave 68a8c44fbab09bc401507cf8365ed87248918783 0 1604245765000 1 connected
  dd44b3f386865561c30219aae9466cb2959a9df5 192.168.30.137:6380@16380 slave da30388b4802e0aa64ef340b6641eaf40e20c20a 0 1604245767508 5 connected
  797fdce256f9d0c90342716087cc028a2a0c6a14 192.168.30.140:6380@16380 master,fail - 1604245707940 1604245705000 7 disconnected 5461-10922
  8b96e6e5e7be7a31fd3ccbf7055af07990f2ef90 192.168.30.139:6379@16379 slave,fail 797fdce256f9d0c90342716087cc028a2a0c6a14 1604245665595 1604245661000 7 disconnected
  da30388b4802e0aa64ef340b6641eaf40e20c20a 192.168.30.140:6379@16379 master - 0 1604245766000 5 connected 10923-16383
  68a8c44fbab09bc401507cf8365ed87248918783 192.168.30.137:6379@16379 myself,master - 0 1604245765000 1 connected 0-5460
  
  127.0.0.1:6379> set t3 v3
  (error) CLUSTERDOWN The cluster is down
  ```